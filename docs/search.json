[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sam Weiner is an aspiring Data Scientist who is excited to bring creativity and analytical know-how to your data science team. I am a current masters student at the Institute for Advanced Analytics, where I am studying data science and technical communication to be able to solve complex problems and share results answers with a wide audience.\nI became interested in data science because of my growing curiosity. In my undergraduate philosophy courses, I enjoyed learning about and debating the answers to fascinating philosophical questions, but as my interests broadened from theoretical questions to practical ones, I realized that the skills I had as a philosopher were insufficient to answering real-word questions. When I heard about data science for the first time, I knew that becoming a data scientist was going to be my path forward to gaining a better understanding of our world. My curiosity has only grown as I have embarked on this journey, and I am so excited to dive into all of the data I can get my hands on to deepen my knowledge of the relationships and events that make up our endlessly fascinating world!\nMy journey has led me to become proficient in R, Python, and SQL. I have become intimately familiar with statistical learning techniques in the inferential world - linear regression, logistic regression, decision trees, causal inference - as well as in the predictive world - random forests, XGBoost, neural networks. I am leveraging Tableau and ggplot2 to create intuitive data visualizations to communicate data analysis to a broad audience.\nI am still passionate about big philosophical questions, and I love to have stimulating conversations with my friends or strangers! In my free time, you might see me watching college wrestling (Go RU!) or obsessing over the perfect hummus recipe."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nRutgers University | New Brunswick, NJ\nB.A. in Philosophy | Sept 2018 - May 2021\nInstitute for Advanced Analytics, North Carolina State University | Raleigh, NC\nM.S. Candidate in Analytics | June 2022 - Current"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dialectic with Data",
    "section": "",
    "text": "Mapping Aviation Data\n\n\n\n\n\n\n\nmaps\n\n\nrstats\n\n\napi\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nSam Weiner\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday - 12/20\n\n\n\n\n\n\n\ntidytuesday\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSam Weiner\n\n\n\n\n\n\n  \n\n\n\n\nR resources for Practicum\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nSam Weiner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday - World Cup Edition\n\n\n\n\n\n\n\ntidytuesday\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nSam Weiner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSam Weiner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/R-resources/index.html",
    "href": "posts/R-resources/index.html",
    "title": "R resources for Practicum",
    "section": "",
    "text": "Hello again! I’m excited to put another post up on Dialectic with Data where I will be brain dumping some of the awesome R resources I have ran across in my journey learning and loving R! This post is was specifically inspired by my practicum team because I have enjoyed sharing these resources with them over time, but many of them have been lost deep in the depths of our slack channel. So, I am going to be cataloging many of the things that I have shared with them, and I will also be adding new resources as I come upon them for them and you!"
  },
  {
    "objectID": "posts/R-resources/index.html#general-r-tips",
    "href": "posts/R-resources/index.html#general-r-tips",
    "title": "R resources for Practicum",
    "section": "\n1 General R tips",
    "text": "1 General R tips\n\n1.1 R for Data Science\nMy biggest and most valuable tip to learning R is read R for Data Science by Hadley Wickham and Garret Grolemund.This book has so much knowledge stored inside it. It primarily teaches the tidyverse and its wonderful array of tools for exploratory data analysis and data visualization. It has been a bible for me, and I see myself turning back to it more often than one would think for a self-classified introductory book. The second version is currently under development but available to read here.\n\n1.2 Rstudio Projects\nRstudio projects are a convenient and efficient way of organizing all of the code, data, and visualizations that make up a data science project. The biggest efficiency boost in my experience is that your working directory automatically updates to the folder where your Rstudio project lives, so all of you code and visualizations are automatically saved in the same place. Additionally, any data that you have in, for example, a csv file can be easily uploaded to your environment without having to mess with the working directory. Other important features that are a bit more advanced revolve around Rstudio projects ability to improve reproducibility and use version control.\n\nResources for learning more about Rstudio Projects\n\nIntro to Rstudio Projects\nRstudio and reproducibility\nRstudio and git\n\n\n\n1.3 Shortcuts, Tips, and Tricks\nThe Rstudio IDE is probably the best IDE (integrated development environment) for using R and conducting data science tasks in general. It has stiff competition from the likes of Visual Studio Code, but there are a few tips and tricks that I am going to show you for Rstudio that might make you think twice before switching to a different IDE.\n\n\nAlt + -\n\nShortcut for inserting the assignment operator <-\n\n\n\n\nCtrl + Shift + M\n\nShortcut for inserting the pipe magrittr operator %>% commonly used in the tidyverse\nNote: As of R 4.1.0, R has a built in pipe operator |> which works similarly to the magrittr pipe but has same slight nuances. See here and here for more information on the built in pipe operator.\n\n\n\n\nCtrl + Shift + R\n\nCreates a comment section header in an R source file which is collapsible and also creates an outline to easily navigate your code file.\n\n\n\nCtrl + Shift + Alt + M\n\nRenames a variable in the entire file. Just highlight the variable to be renamed, use the shortcut, and type the new name. Easy as that!\nUsing Ctrl + F and using replace is another option which is useful when you don’t want to rename every instance of a variable in the file.\n\n\nCode Snippets\n\nCode Snippets are ways to insert commonly used code structures (or headers) in an R file. Under the Tools toolbar, go to Edit Code Snippets. There you will see all the code snippets pre-built in Rstudio and and editor for adding new ones. I use this to add a header structure to new scripts where I can fill in information about the script for my teammates to read when they use it.\n\n\nMultiple Cursors\n\nThis was a tip that I was sorely missing from my time using VS Code until I stumbled upon this simple shortcut in Rstudio! Just by holding down the Alt button and clicking or draging with the mouse, you’ll have more cursors than you know what to do with. More awesome shortcuts can be found in this awesome article by Appsilon\n\n\n\n\n1.4 Quarto\nQuarto is Rstudio’s (now called Posit) new Rmarkdown replacement. The biggest change from Rmarkdown to Quarto is that you can use Quarto in Jupyter Notebooks! Posit (formerly Rstudio) is broadening it’s presence in the data science community by engaging with python users in addition to R users. The hope, says Posit, is to create easy tools for cross-language collaboration so that researchers and data scientists who have different languages of choice can work together with ease. From what I have seen, many data scientists who might have preferred R as a language have been pressured to use python because of its large presence in the data science community. This tool is hopefully going to create a pathway for R users to stay R users without complicating team workflows in a python-dominant environment. In addition, Quarto has some awesome features such as helping users easily create documents, presentation, or even blogs (like this one!). So far, it has been very intuitive and the documentation has been very helpful. You can find information at quarto.org."
  },
  {
    "objectID": "posts/R-resources/index.html#packages",
    "href": "posts/R-resources/index.html#packages",
    "title": "R resources for Practicum",
    "section": "\n2 Packages",
    "text": "2 Packages\nNow lets get to packages! I am going to be listing packages that I think are really helpful (and really cool) for working with data and doing modeling in R.\n\n2.1 Tidyverse\n\n\nggplot2\n\nThis package contains the best tools for taking data and making a stellar visualization out of it. It is extremely versatile and at times very complicated. Later on I will be sharing more about ggplot2 and provide some resources for making ggplot2 super easy to understand.\n\n\n\ndplyr\n\nThis package has the tools you need to manipulate data. Mutate is useful for feature engineering and summarise is great for calculating summaries of your data set. Using the piping syntax makes this package so powerful. My favorite way of using this package is to dig into the data and pipe what I want into a ggplot!\n\n\n\nreadr\n\nThis package makes importing data easy as can be.\n\n\n\npurrr\n\nThis package contains the tidyverse’s toolkit for functional programming. The iteration chapter in R for Data Science is a great place to start if you are new to functional programming. Another great resources I used is purrr tutorial by Jenny Bryan. Functional programming has been very hard for me to learn, but these two resources have really helped me wrap my brain around the concept, and it has already impacted my projects enormously.\n\n\n\nstringr\n\nstringr is the handiest way of working with strings in R. It has everything under the sun when it comes to manipulating and cleaning character data.\n\n\n\nforcats\n\nFactors are a very common data type in R, but they can be tricky at times. This package has some awesome tools for working with factors that will really come in handy if you use factors a lot.\n\n\n\n2.2 Tidymodels\nTidymodels is a standalone universe which has everything you need to do machine learning in R. The tidymodels project is headed by the same person that create CARET years back, Max Kuhn, and the goal of tidymodels is the same as CARETS goal: To create a unified interface for machine learning in R. The package makes use of R’s rich ecosystem of machine learning model packages but standardizes the interface across all those implementations to make the switch between models seamless. The amount of packages and tools in the tidymodels universe is large and there is too much to say for this blog post. However, I do plan on showing how tidymodels is used in a future blog post, so stay tuned! In the meantime, I am going to point you to some amazing resources if you want to get started now. - Tidy Modeling with R is a free book written by Max Kuhn and Julia Silge which is the best resource for getting started with tidymodels and it also has some great information on machine learning in general! - tidymodels.org has some amazing content which will be the preferred resource for people who want a shorter format introduction to tidymodels. - juilasilge.com is the blog of one of the co-authors of Tidy Modeling with R and she has some great blog posts and youtube videos where she implements the tidymodels packages and gives great explanations of what she is doing.\n\n2.3 Awesome Packages\nThe Awesome Packages section is going to be a growing list of packages that I find really cool because they have either changed and/or improved the way I use R as a data science student.\n\n2.3.1 broom\nbroom is a super cool R package. One of the most annoying aspects of using R for data science is trying to extract the output from the model object. Summaries of the model object are easy enough to get using summary(), but to extract the components of model like the coefficients, pvalues, or Rsquared value to use for other operations means digging into the model object and trying and failing to use the right index to find it. broom makes that task no more. By using the tidy() function on supported model objects, the output is represented as a data frame which makes extracting what you want so much easier. Rsquared and other metric vaules can be found by using glance() and augment() can add output like predictions from a new dataset back into the data set from whence it came.\n\nlibrary(tidyverse)\nlibrary(broom)\nmpg_lm <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n\n\n\nsummary()\ntidy()\nglance()\naugment()\n\n\n\n\nsummary(mpg_lm) # Normal summary output\n\n\nCall:\nlm(formula = mpg ~ wt + hp + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\nwt          -3.16697    0.74058  -4.276 0.000199 ***\nhp          -0.01804    0.01188  -1.519 0.140015    \ncyl         -0.94162    0.55092  -1.709 0.098480 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n\n\n\n\n\ntidy(mpg_lm) # tidy() returns coefficients and pvalues\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  38.8       1.79       21.7  4.80e-19\n2 wt           -3.17      0.741      -4.28 1.99e- 4\n3 hp           -0.0180    0.0119     -1.52 1.40e- 1\n4 cyl          -0.942     0.551      -1.71 9.85e- 2\n\n\n\n\n\nglance(mpg_lm) # glance returns model metrics\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.843        0.826  2.51    50.2 2.18e-11     3  -72.7  155.  163.    177.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\n\n\n# augment with 'newdata = ' returns entire dataset plus fitted and residual values\naugment(mpg_lm, newdata = mtcars)\n\n# A tibble: 32 × 14\n   .rownames     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with 22 more rows, and 2 more variables: .fitted <dbl>, .resid <dbl>\n\n# augment without newdata uses training data and returns more metrics\naugment(mpg_lm)\n\n# A tibble: 32 × 11\n   .rowna…¹   mpg    wt    hp   cyl .fitted .resid   .hat .sigma .cooksd .std.…²\n   <chr>    <dbl> <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n 1 Mazda R…  21    2.62   110     6    22.8 -1.82  0.0750   2.53 1.15e-2  -0.754\n 2 Mazda R…  21    2.88   110     6    22.0 -1.01  0.0582   2.55 2.67e-3  -0.416\n 3 Datsun …  22.8  2.32    93     4    26.0 -3.16  0.0856   2.48 4.05e-2  -1.32 \n 4 Hornet …  21.4  3.22   110     6    20.9  0.464 0.0533   2.56 5.08e-4   0.190\n 5 Hornet …  18.7  3.44   175     8    17.2  1.53  0.110    2.54 1.29e-2   0.647\n 6 Valiant   18.1  3.46   105     6    20.3 -2.15  0.0699   2.52 1.48e-2  -0.888\n 7 Duster …  14.3  3.57   245     8    15.5 -1.19  0.118    2.55 8.53e-3  -0.506\n 8 Merc 24…  24.4  3.19    62     4    23.8  0.636 0.157    2.55 3.55e-3   0.276\n 9 Merc 230  22.8  3.15    95     4    23.3 -0.496 0.152    2.56 2.06e-3  -0.214\n10 Merc 280  19.2  3.44   123     6    20.0 -0.789 0.0469   2.55 1.27e-3  -0.322\n# … with 22 more rows, and abbreviated variable names ¹​.rownames, ²​.std.resid\n\n\n\n\n\n\n2.3.2 janitor\njanitor is a key package because it does something that, while we might think is important, is not all that interesting, but is a such a quality of life boost for a data scientist. Cleaning dirty data is numerous ways. Cleaning dirty data is a necessary part of the process to extracting value out a data set, but it is such a time sink. Janitor comes in to provide a myriad of helpful functions to get data people working in the data instead of on the data.\n\nlibrary(janitor)\n\n# Data frame with bad column names\ntest <- tribble(\n  ~`Customer ID`, ~RegionCode, ~ElePhanTIntHeRoOm,\n  242, \"A\", 24,\n  3422, \"B\", 353\n)\n\ntest\n\n# A tibble: 2 × 3\n  `Customer ID` RegionCode ElePhanTIntHeRoOm\n          <dbl> <chr>                  <dbl>\n1           242 A                         24\n2          3422 B                        353\n\n# clean_names() takes care of most bad column names in most forms\n# Not always going to work but its great for 99% of the data files you'll encounter\njanitor::clean_names(test)\n\n# A tibble: 2 × 3\n  customer_id region_code ele_phan_t_int_he_ro_om\n        <dbl> <chr>                         <dbl>\n1         242 A                                24\n2        3422 B                               353\n\n\n\n2.3.3 skimr\nskimr is your best friend when beginning exploratory data analysis. Using the skim() function on a data frame, a really easy to understand output will be generated containing summary statistics for all of your columns. The information will include variable type, percent missing, 0th, 25th, 50th, 75th, 100th percentile value for continuous variables, mean, standard deviation, and more! I think that you will love this tool once you start to use it!\n\nlibrary(skimr)\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\n\n2.3.4 reprex\nWe all know what it’s like to feel completely baffled when writing code. At least I can say I have felt completely baffled when writing code and I have felt stuck countless times. And when I am baffled, I go to the internet to find the cure. If I ever have to ask a question on sites like stack overflow or github to ask for help, I always use reprex to help me create a minimally reproducible example. A minimally reproducible example is simple piece of code that replicates your problem so that other knowledgeable people can help you diagnose your issue. Asking questions with reprex gives you the best chance to finding an answer to your question because you have helped the internet help you by giving them the exact situation you are facing.\nCreate a reproducible example of the error that you are encountering. For instance, lets say you are trying to create three boxplots looking at the distribution of some continuous variable across the values of a categorical variable, but the output doesn’t look right. Let’s use the mtcars dataset to illustrate this issue.\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nOnce we have recreated our issue in a simple and easy to follow example, we highlight all of the code that was used to create the example, including the libraries. Then, we run the reprex() function which will nicely format our code with any errors or warnings that we may have gotten and copy that to our clipboard.\n\nlibrary(reprex)\n\nreprex()\n\nThe output will look like this when we upload our code to stackoverflow or github:\n\n\nIt even includes the plot as image which will show up when we post to stack overflow or github!\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n#> Warning: Continuous x aesthetic\n#> ℹ did you forget `aes(group = ...)`?\n\nCreated on 2023-01-08 with reprex v2.0.2\nAlso, by adding factor() around cyl, we can get the plot we were looking for!\n\nggplot(mtcars, aes(factor(cyl), mpg)) +\n  geom_boxplot()\n\n\n\nCorrect way to create boxplot\n\n\n\n\n\n2.3.5 here\nSharing code is an important part of collaborative data science projects which is common in the field. To be able make sure code is able to be run by others when shared, coders must be cognizant of not hard-coding file paths into their scripts. Other people will 99.99% of the time not have the same file structure as you do, and hard coding a file path will make that script only usable by you until all the file paths are changed by the person you shared it with. We don’t want to make sharing code difficult, so I am hoping that you use the here package to help you create relative file paths! The here package makes creating relative file paths as simple as can be and using it will make sharing code will be hindrance free.\n\nlibrary(here)\n\nhere() starts at C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\n\n\nCalling here() will output the top of the project directory\n\nhere()\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\"\n\n\nBy adding folder names in quotation marks, we can easily dive deeper into our project directory.\n\nhere(\"posts\")\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts\"\n\nhere(\"posts\", \"R-resources\")\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts/R-resources\"\n\n\n\n2.3.6 todor\nWorking on big projects means that there are always multiple tasks that are being managed at once. The todor package lets you create searchable comments within your R scripts or even entire projects to help you keep track of your tasks. Never forget again with todor!\nUsing todor is as easy writing a comment in R and using a keyword like TODO at the beginning.\n\nlibrary(todor)\n\n# TODO Show an example of todor\n\n<!-- TODO Change this section. -->\nWhen we call the todor() function, we get output like this. As you might have noticed, todor can detect HTML comments as well!\n\n\ntodor\n\n\n\n2.3.7 styler\nSometimes when we’re coding, readability is not on the front of our mind. Use the styler package to help make your scripts more readable so that you know what it’s doing the next time you come back to it. All you have to do is run one function, and the package non-invasively stylizes your code to be in a standard format so that your code looks consistent across projects.\nLet’s say we have code like this. Quite difficult to read through, no?\niris |> \n  filter(Sepal.Length > 5.1) |> select(Petal.Width, Petal.Length) |> group_by(Petal.Length)|> summarise(mean = Petal.Width)\nAfter running styler::style_active_file(), our code is reformmated and much more readable in my opinion.\niris |>\n  filter(Sepal.Length > 5.1) |>\n  select(Petal.Width, Petal.Length) |>\n  group_by(Petal.Length) |>\n  summarise(mean = Petal.Width)"
  },
  {
    "objectID": "posts/R-resources/index.html#ggplot2",
    "href": "posts/R-resources/index.html#ggplot2",
    "title": "R resources for Practicum",
    "section": "\n3 ggplot2",
    "text": "3 ggplot2\nggplot2 is the primary visualization package in R. It allows for extreme creativity when turning data into visualizations. Checkout the #tidytuesday hashtag on twitter to see some of the crazy impressive visualizations made with ggplot2! ggplot2 is not a package that is simple to master, however. For those beginning their journey with ggplot2, I recommend R for Data Science’s chapter on data visualization which gives a great explanation behind the design of ggplot2 and how it is meant to be used. If you need a reference for ggplot2 code, R Graphics Cookbook is a great resource as are the R Graph Gallery and R charts. I find that sometimes I need to understand how ggplot2 takes data and makes visuals out of it, which is what R for Data Science is great for, and other times I just need a resource to show me all the cool ways I can represent my data, which is what those other three resources are there for.\n\n3.1 Extensions\nOne major benefit of ggplot2 being the default visualization package in R is that it has been building blocks of choice for so many R programmers create new visualization capabilities in R and ggplot2. There are hundreds of extension packages for ggplot2 that add onto its functionality in incredible ways. For instance, gganimate creates the ability for you to have animated graphics in R! And the best part is that these extensions are usually very easy to learn because they all use the same ggplot2 mechanics as I was saying earlier. A full list of registered extensions can be found here. One really exquisite ggplot extension I would like to highlight is esquisse. This package creates a tableau-like interface inside of Rstudio, so that we can interactively plot our data! It has been a game changer for me when doing EDA.\n\n\nesquisse example\n\n\n\n3.2 Fonts\nOne way that people like to modify their visualizations from base ggplot2 is by modifying text fonts. Packages like extrafont and showtext make doing this extremely easy. hrbrthemes is an example of pre-built themes that R users have created that take try to enhance ggplot2 output in this way.\nHere’s what we can do with showtext.\nFirst lets look at standard ggplot2 output.\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\nUsing showtext, we can import fonts from google fonts with a simple function and then use some ggplot2 options to include theme in our plot.\n\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Dancing Script\", \"dance\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Covered By Your Grace\", \"grace\")\nfont_add_google(\"Rock Salt\", \"rock\")\n\nshowtext_auto()\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  xlab(\"Sepal Length\") +\n  ylab(\"Sepal Width\") +\n  ggtitle(\"Sepal Width by Sepal Length Colored by Species\") +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(family = \"gochi\", size = 20), \n    axis.title.y = element_text(family = \"bell\", size = 20), \n    plot.title = element_text(family = \"grace\", size = 20), \n    legend.text = element_text(family = \"rock\", size = 20), \n    legend.title = element_text(\"dance\", size = 20),\n    axis.text = element_text(size = 20)\n    )\n\n\n\n#This function makes it so that any future plots are reverted back to the \n#original settings\nshowtext_end()\n\nWe used four different fonts in one ggplot! I’ll admit it’s not the nicest looking plot I’ve ever made, but the point stands that you can customize ggplots to your hearts content using packages like showtext.\n\n3.3 Scales\nScaling data in a visualization is a crucial part of effective data communication. If the scale of the data is not clear, then any inferences from that communication will either be unclear or misled. The scales package provides some awesome tooling to make very clear and descriptive plot scales.\nHere’s a bad example but an example nonetheless. Using two scales functions, label_dollar() and label_percent, we changed the axis labels of the previous graph to be more descriptive. There are many more powerful functions in the scales package that I am sure will benefit your data communication tasks.\n\nlibrary(scales)\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 20),\n        axis.title = element_text(size = \"20\"),\n        legend.title = element_text(size = 20),\n        legend.text = element_text(size = 20))"
  },
  {
    "objectID": "posts/R-resources/index.html#databases",
    "href": "posts/R-resources/index.html#databases",
    "title": "R resources for Practicum",
    "section": "\n4 Databases",
    "text": "4 Databases\nWorking with databases in R is not terribly complicated. The DBI package is very useful when connecting to and querying databases. But there are some really cool tools for working with databases in that I you’re going to like.\n\n4.1 dbplyr\ndbplyr is a tidyverse package that converts dplyr code into SQL queries. Yes, you read that right. I need to note right away, however, that it does not aim to replace SQL in your data science workflow; while it is robust, it is not a complete replacement. With that said, when I have been coding in R for a while and have dplyr code right at the front of mind, it has been great to just whip up some quick code to enable some more EDA with dbplyr. It is a great addition to any R users workflow if they work with databases frequently."
  },
  {
    "objectID": "posts/R-resources/index.html#causal-inference",
    "href": "posts/R-resources/index.html#causal-inference",
    "title": "R resources for Practicum",
    "section": "\n5 Causal Inference",
    "text": "5 Causal Inference\nCausal inference is a topic that I have been learning about a lot lately. As someone who does not have a formal background in statistics or epidemiology, it has been important to me to find resources that teach causal inference in terms that I can understand. Here is a list of resources and R packages that can help you if you are interested in causal inference.\n\nCausal Inference in R is a ongoing book project by Malcolm Barrett, Lucy D’Agostino McGowan, and Travis Gerke. It contains some amazing information at the moment and it will only get better as the authors work on it.\nCausal Inference for The Brave and True by Matheus Facure Alves is a book that has great explanations of causal inference techniques and concepts and everything coding related is written in python for those interested in a python implementation of causal inference!\nCausal Inference: What If? by Miguel A. Hernán and James M. Robins is causal inference bible. It is much heavier than the resources above and the focus is purely on the methodology of causal inference. However R code which follows part 2 of the book chapter by chapter can be found here.\nWorkshop: Causal Inference in R is a video workshop by Malcolm Barrett and Lucy D’Agostino McGowan. The workshop has been updated since the video but no new video has surfaced that I could find. The new workshop can be found on github here with the slides and exercise rmarkdown files. The solutions to the exercises can be found here.\nMatchIt is a package that creates matches based on observational data. It has awesome vignettes on its website that explains matching for causal inference in detail and has many method options to implement for matching.\nggdag is a way to plot directed acyclic dags (DAGs) in R using more a ggplot2-like interface as opposed to daggity.\npropensity helps calculate propensity scores and weights for a wide variety of research questions.propensity is under very early development.\ntipr After fitting your model, you can determine the unmeasured confounder needed to tip your analysis.\nhalfmoon The goal of halfmoon is to cultivate balance in propensity score models."
  },
  {
    "objectID": "posts/tidyt_1220/index.html",
    "href": "posts/tidyt_1220/index.html",
    "title": "Tidy Tuesday - 12/20",
    "section": "",
    "text": "Hello everyone! Tidy Tuesday released this data set on December 20th, 2022 and it is so interesting. It was collected as a part of a data science capstone project where national weather data was acquired to learn which areas of the U.S. struggle with weather prediction and the possible reasons why. Specifically, the project focused on the error in high and low temperature forecasting.\nThe data includes 16 months of daily forecasts and observations from 167 cities, as well as a separate data frame of information about those cities and some other American cities.\nSo let’s dive into this data and learn something about forecasting weather!\n\nlibrary(tidyverse)\nlibrary(skimr)\n#Tidyverts universe of packages\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(lubridate)\n\n\nweather_forecasts <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/weather_forecasts.csv\")\ncities <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/cities.csv\")\noutlook_meanings <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/outlook_meanings.csv\")\n\nThe weather forecasts data set contains daily high and daily low temperature forecasts 48, 36, 24, and 12 prior to every date in the data set. The data also contains the observed high and low temperature for that date. (Note: some dates have missing data for observed temperature.)\nHere’s a glimpse at the column names, the number of rows, and the first couple of values in the weather forecasts data set.\n\nweather_forecasts |> glimpse()\n\nRows: 651,968\nColumns: 10\n$ date                  <date> 2021-01-30, 2021-01-30, 2021-01-30, 2021-01-30,…\n$ city                  <chr> \"ABILENE\", \"ABILENE\", \"ABILENE\", \"ABILENE\", \"ABI…\n$ state                 <chr> \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", …\n$ high_or_low           <chr> \"high\", \"high\", \"high\", \"high\", \"low\", \"low\", \"l…\n$ forecast_hours_before <dbl> 48, 36, 24, 12, 48, 36, 24, 12, 48, 36, 24, 12, …\n$ observed_temp         <dbl> 70, 70, 70, 70, 42, 42, 42, 42, 29, 29, 29, 29, …\n$ forecast_temp         <dbl> NA, NA, NA, 70, NA, NA, 39, 38, NA, NA, NA, 30, …\n$ observed_precip       <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ forecast_outlook      <chr> NA, NA, NA, \"DUST\", NA, NA, \"DUST\", \"SUNNY\", NA,…\n$ possible_error        <chr> \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", …\n\n\nThe data contains multiple time series, one for each combination of city, state, high_or_low, and forecast_hours_before. I think that looking into how the different forecast times compare in accuracy will be interesting to look at, as well as whether forecast accuracy differs at different times in the year.\nI will be using the tsibble package for this data which is useful when working with time series data. The first of which is that it enables us contain multiple time series in the same data structure, a time series tibble, and this allows us run models or check for certain features on multiple series simultaneously. If you want to learn more about time series analysis and how to use the tidyverts packages, I highly recommend reading Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos. It is a very detailed book and covers a lot of ground in the field of time series.\n\nforecast_tsbl <- weather_forecasts |>\n  mutate(city = factor(city),\n         state = factor(state),\n         high_or_low = factor(high_or_low),\n         forecast_hours_before = factor(forecast_hours_before)) |> \n  as_tsibble(key = c(city, state, high_or_low, forecast_hours_before), index = date)\n\nI am going to be subsetting the data to look at one city, Newark, NJ. This visualization is a time plot of the observed daily high temperature for Newark, NJ from January 30th, 2021 to June 1st, 2022.\n\nforecast_tsbl |> \n  filter(city == \"NEWARK\", high_or_low == \"high\", forecast_hours_before == 12) |>\n  autoplot(observed_temp)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nSliding Window\nWe will be looking at the rolling mean absolute error between forecasted high and low temperatures versus observed high and low temperatures across the different temperature forecasts.\nI am using a 30 day window around each date to compute the mean. I am also ignoring dates with missing values.\n\nslide_tsbl <- forecast_tsbl |> \n  filter(city == \"NEWARK\") |> \n  mutate(error = forecast_temp - observed_temp) |> \n  mutate(mean_temp_error = slider::slide_dbl(error,\n                                              ~mean(abs(.), na.rm = TRUE), \n                                              .before = 15,\n                                              .after = 15, \n                                              .complete = TRUE))\n\nThis plot shows the mean absolute error across the dates for the high temperature forecast across forecast times.\n\n\nCode\nslide_tsbl |> \n  filter(high_or_low == \"high\") |> \n  ggplot() +\n  geom_line(aes(x = date, y = mean_temp_error)) + \n  facet_grid(vars(high_or_low, forecast_hours_before)) +\n  xlab(\"Date\") +\n  ylab(\"Mean Error\")\n\n\n\n\n\nThis plot shows the average error across the dates for the low temperature forecast across forecast times.\n\n\nCode\nslide_tsbl |> \n  filter(high_or_low == \"low\") |> \n  ggplot() +\n  geom_line(aes(x = date, y = mean_temp_error)) + \n  facet_grid(vars(high_or_low, forecast_hours_before))  +\n  xlab(\"Date\") +\n  ylab(\"Mean Error\")\n\n\n\n\n\nWe can see the overall mean absolute error for each forecast.\n\nforecast_tsbl |> \n  as_tibble() |> \n  filter(city == \"NEWARK\", state == \"NJ\") |>  \n  mutate(error = forecast_temp - observed_temp) |>\n  group_by(forecast_hours_before) |> \n  summarise(MAE = mean(abs(error), na.rm = T))\n\n# A tibble: 4 × 2\n  forecast_hours_before   MAE\n  <fct>                 <dbl>\n1 12                     1.98\n2 24                     2.12\n3 36                     2.29\n4 48                     2.41"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello World!\nI am excited to start this blog where I will share my journey learning data science. I hope to share data analysis that I have conducted using the awesome tool-kit I have gained at the Institute for Advanced Analytics, some R packages that I think are cool, and my thoughts on the field of data science as a whole. Cheers to a new adventure!\n- Sam"
  },
  {
    "objectID": "posts/world-cup/index.html",
    "href": "posts/world-cup/index.html",
    "title": "Tidy Tuesday - World Cup Edition",
    "section": "",
    "text": "Tidy Tuesday is an awesome initiative by the R for Data Science Community where data sets are published every week for the R community share any analysis and visualizations that can be conjured from the data!\n\n\nThis data set contains data on every World Cup since 1930.\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tidymodels)\ntheme_set(theme_light())\n\n\n\n\n\ntt <- tt_load(\"2022-11-29\")\ntt\n\nThis episode of Tidy Tuesday contains two data sets. wcmatches (matches from now on) contains 900 rows, one for each game played on the World Cup stage. It has variables for who played who, what stage (or round) of the tournament the game was played, who won, and the goals scored by each team.\n\nmatches <- tt$wcmatches\nglimpse(matches)\n\nRows: 900\nColumns: 15\n$ year           <dbl> 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1…\n$ country        <chr> \"Uruguay\", \"Uruguay\", \"Uruguay\", \"Uruguay\", \"Uruguay\", …\n$ city           <chr> \"Montevideo\", \"Montevideo\", \"Montevideo\", \"Montevideo\",…\n$ stage          <chr> \"Group 1\", \"Group 4\", \"Group 2\", \"Group 3\", \"Group 1\", …\n$ home_team      <chr> \"France\", \"Belgium\", \"Brazil\", \"Peru\", \"Argentina\", \"Ch…\n$ away_team      <chr> \"Mexico\", \"United States\", \"Yugoslavia\", \"Romania\", \"Fr…\n$ home_score     <dbl> 4, 0, 1, 1, 1, 3, 0, 0, 1, 6, 1, 0, 0, 4, 3, 6, 6, 4, 2…\n$ away_score     <dbl> 1, 3, 2, 3, 0, 0, 4, 3, 0, 3, 0, 1, 4, 0, 1, 1, 1, 2, 3…\n$ outcome        <chr> \"H\", \"A\", \"A\", \"A\", \"H\", \"H\", \"A\", \"A\", \"H\", \"H\", \"H\", …\n$ win_conditions <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ winning_team   <chr> \"France\", \"United States\", \"Yugoslavia\", \"Romania\", \"Ar…\n$ losing_team    <chr> \"Mexico\", \"Belgium\", \"Brazil\", \"Peru\", \"France\", \"Mexic…\n$ date           <date> 1930-07-13, 1930-07-13, 1930-07-14, 1930-07-14, 1930-0…\n$ month          <chr> \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\",…\n$ dayofweek      <chr> \"Sunday\", \"Sunday\", \"Monday\", \"Monday\", \"Tuesday\", \"Wed…\n\n\nworldcups (wcups from now on) has information about each world cup on a high level. It contains information like the name of the host country each World Cup, the names of the top four finishers, total tournament goals scored, and the number of teams, games, and attendants.\n\nwcups <- tt$worldcups\nglimpse(wcups)\n\nRows: 21\nColumns: 10\n$ year         <dbl> 1930, 1934, 1938, 1950, 1954, 1958, 1962, 1966, 1970, 197…\n$ host         <chr> \"Uruguay\", \"Italy\", \"France\", \"Brazil\", \"Switzerland\", \"S…\n$ winner       <chr> \"Uruguay\", \"Italy\", \"Italy\", \"Uruguay\", \"West Germany\", \"…\n$ second       <chr> \"Argentina\", \"Czechoslovakia\", \"Hungary\", \"Brazil\", \"Hung…\n$ third        <chr> \"USA\", \"Germany\", \"Brazil\", \"Sweden\", \"Austria\", \"France\"…\n$ fourth       <chr> \"Yugoslavia\", \"Austria\", \"Sweden\", \"Spain\", \"Uruguay\", \"W…\n$ goals_scored <dbl> 70, 70, 84, 88, 140, 126, 89, 89, 95, 97, 102, 146, 132, …\n$ teams        <dbl> 13, 16, 15, 13, 16, 16, 16, 16, 16, 16, 16, 24, 24, 24, 2…\n$ games        <dbl> 18, 17, 18, 22, 26, 35, 32, 32, 32, 38, 38, 52, 52, 52, 5…\n$ attendance   <dbl> 434000, 395000, 483000, 1337000, 943000, 868000, 776000, …"
  },
  {
    "objectID": "posts/world-cup/index.html#world-cup-dataset",
    "href": "posts/world-cup/index.html#world-cup-dataset",
    "title": "Tidy Tuesday - World Cup Edition",
    "section": "World Cup dataset",
    "text": "World Cup dataset\nAs was mentioned above, the World Cup dataset contains more general information about each World Cup.\n\nwcups\n\n# A tibble: 21 × 10\n    year host        winner      second third fourth goals…¹ teams games atten…²\n   <dbl> <chr>       <chr>       <chr>  <chr> <chr>    <dbl> <dbl> <dbl>   <dbl>\n 1  1930 Uruguay     Uruguay     Argen… USA   Yugos…      70    13    18  434000\n 2  1934 Italy       Italy       Czech… Germ… Austr…      70    16    17  395000\n 3  1938 France      Italy       Hunga… Braz… Sweden      84    15    18  483000\n 4  1950 Brazil      Uruguay     Brazil Swed… Spain       88    13    22 1337000\n 5  1954 Switzerland West Germa… Hunga… Aust… Urugu…     140    16    26  943000\n 6  1958 Sweden      Brazil      Sweden Fran… West …     126    16    35  868000\n 7  1962 Chile       Brazil      Czech… Chile Yugos…      89    16    32  776000\n 8  1966 England     England     West … Port… Sovie…      89    16    32 1614677\n 9  1970 Mexico      Brazil      Italy  West… Urugu…      95    16    32 1673975\n10  1974 Germany     West Germa… Nethe… Pola… Brazil      97    16    38 1774022\n# … with 11 more rows, and abbreviated variable names ¹​goals_scored,\n#   ²​attendance\n\n\nWe can count the number of total goals scored in a World Cup and rank them by which tournament had the most goals scored.\n\nwcups %>% \n  count(year, wt = goals_scored, sort = TRUE)\n\n# A tibble: 21 × 2\n    year     n\n   <dbl> <dbl>\n 1  1998   171\n 2  2014   171\n 3  2018   169\n 4  2002   161\n 5  2006   147\n 6  1982   146\n 7  2010   145\n 8  1994   141\n 9  1954   140\n10  1986   132\n# … with 11 more rows\n\n\nLet’s also visualize the relationship between number of goals scored and the number of matches played in each World Cup.\n\nwcups %>% \n  ggplot(aes(year, goals_scored)) +\n  geom_line() +\n  geom_line(aes(year, games))"
  },
  {
    "objectID": "posts/airline_map/index.html",
    "href": "posts/airline_map/index.html",
    "title": "Mapping Aviation Data",
    "section": "",
    "text": "Hello and welcome back to Dialectic with Data! In this installment, I am going to be working with data from the AviationStack API. This API can let us retrieve real time flight data from all over the world, and I thought that it would be interesting to visualize the flights that are currently on their way to a specific airport.\nTo recreate this project yourself, you can sign up for a free account with AviationStack here. The free account lets us make 100 api calls a month which is really awesome!\nFor this project, I am going to be working in R, which has a great package for working with api calls, httr. This package lets us easily assemble queries.\n\nCodelibrary(tidyverse)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(airportr)\nlibrary(gt)\n\n\n\n\n\n\nThe first step for working with APIs is to get our API key into our R session. Generally, it is not a very good practice to have our API key written out in our code. The key is liable to get stolen and abused, and that is something that we want to prevent. If you plan on using the API key a lot, you might want to add it to your .Renviron file. Doing this is very simple and generally safe.\nTo get to your .Renviron file, go to the console and run this code usethis::get_r_environ(). Your .Renviron should pop up and then you can add a line like this: aviation_api = {your_key_here}. Now, whenever you want to retrieve you API key, call Sys.getenv(\"aviation_api\") and your API key will appear.\nIf you only plan on using your API key a couple of times, however, Rstudio has a function that you can use to manually add your API key each time you want to add it to your R environment. Run rstudioapi::askForPassword() and an interactive window will pop up where you can put your key. Just remember to save it to a variable!\napi <- rstudioapi::askForPassword()\n\nFor this project, I am going to be working in R, which has a great package for working with API calls, httr. This package lets us easily assemble queries.\nTo make a call from the API, we are going to use httr::GET(). To construct the call, we put the base URL as a string in the function. Then, in the query parameter, we write a named list using the different parameters we have to or want to add to the call. access_key is where we put our API key and this is a necessary parameter. arr_iata is an optional parameter which is used to get us the information that we want, but it is not necessary to have a successful API call. You can find out more about the different parameters in the AviationStack documentation.\nIn this API call, we are using the real-time flights API endpoint, and we are saying that we want to see flights that are going to be arriving at JFK airport.\nIf you are unfamiliar with the term “IATA”, have no fear! IATA codes are a global standard of references for airports all around the world. They are really easy to find and almost every airport has one.\n\nr <- httr::GET(\"http://api.aviationstack.com/v1/flights\", \n               query = list(access_key = api, arr_iata = \"JFK\"))\n\n\n\n\nOnce our call has been completed, we are going to get back something called a response object. This object contains all the information that we requested. When we run the object in the console, we get some summary information like the full URL of the call (including the API key which is why I’m not showing the output), the time of the call, the size of the request in kB, and the status code.\nThe status code is an important piece of information to look for when working with APIs. A status code of 200 is good. That means everything worked fine and we have our data! A status code of anything else means that we do not have our data, and the reasons could be many. For more information on what other status codes mean, I turn you to the HTTP Status Cats for guidance.\nSo to retrieve the status of our API call, we run httr::status_code(r) and, luckily for us, our request went through without a hitch! (Though this was not always the case when I was writing this article 😥)\n\nhttr::status_code(r)\n\n[1] 200\n\n\n\nTo retrieve the contents of our call, we run httr::content(r) on our response object.\n\nflights <- httr::content(r)\n\nflights is a large list with two main components. pagination and data. We are most interested in what’s contained inside data. pagination contains some descriptive information about the call that we can’t do anything with.\nLet’s take a look at what’s inside data using the str function. The max.level option makes the output a little easier to read.\nAs we can see, data contains lists too! Namely, there are lists called flight date and flight status which each contain one value. Also there are lists called departure, arrival, airline and flight which contain a lot of value and even more lists!\n\nstr(flights$data[[1]], max.level = 4)\n\nList of 8\n $ flight_date  : chr \"2023-01-19\"\n $ flight_status: chr \"scheduled\"\n $ departure    :List of 12\n  ..$ airport         : chr \"Hong Kong International\"\n  ..$ timezone        : chr \"Asia/Hong_Kong\"\n  ..$ iata            : chr \"HKG\"\n  ..$ icao            : chr \"VHHH\"\n  ..$ terminal        : chr \"1\"\n  ..$ gate            : NULL\n  ..$ delay           : NULL\n  ..$ scheduled       : chr \"2023-01-19T03:20:00+00:00\"\n  ..$ estimated       : chr \"2023-01-19T03:20:00+00:00\"\n  ..$ actual          : NULL\n  ..$ estimated_runway: NULL\n  ..$ actual_runway   : NULL\n $ arrival      :List of 13\n  ..$ airport         : chr \"John F Kennedy International\"\n  ..$ timezone        : chr \"America/New_York\"\n  ..$ iata            : chr \"JFK\"\n  ..$ icao            : chr \"KJFK\"\n  ..$ terminal        : chr \"8\"\n  ..$ gate            : NULL\n  ..$ baggage         : NULL\n  ..$ delay           : NULL\n  ..$ scheduled       : chr \"2023-01-19T06:00:00+00:00\"\n  ..$ estimated       : chr \"2023-01-19T06:00:00+00:00\"\n  ..$ actual          : NULL\n  ..$ estimated_runway: NULL\n  ..$ actual_runway   : NULL\n $ airline      :List of 3\n  ..$ name: chr \"Malaysia Airlines\"\n  ..$ iata: chr \"MH\"\n  ..$ icao: chr \"MAS\"\n $ flight       :List of 4\n  ..$ number    : chr \"9210\"\n  ..$ iata      : chr \"MH9210\"\n  ..$ icao      : chr \"MAS9210\"\n  ..$ codeshared:List of 6\n  .. ..$ airline_name : chr \"cathay pacific\"\n  .. ..$ airline_iata : chr \"cx\"\n  .. ..$ airline_icao : chr \"cpa\"\n  .. ..$ flight_number: chr \"844\"\n  .. ..$ flight_iata  : chr \"cx844\"\n  .. ..$ flight_icao  : chr \"cpa844\"\n $ aircraft     : NULL\n $ live         : NULL\n\n\nTo get this data in a nice to read format, how about we turn it into a data frame?\n\ndata_flight <- tibble(data = flights$data)\n\n\ndata_flight |> head()\n\n# A tibble: 6 × 1\n  data            \n  <list>          \n1 <named list [8]>\n2 <named list [8]>\n3 <named list [8]>\n4 <named list [8]>\n5 <named list [8]>\n6 <named list [8]>\n\n\nOK, that’s not nice to read.\nThe issue is that the data is still in a list. We have to get the information out of a list find a way to make it look like a data frame with columns and rows. There is a package in R called, tidyr (which is included in the tidyverse), that will be able to help us. tidyr is full of functions that will help us “rectangle” our data. The function we will be using is unnest_wider(). Let’s see how it works.\nTo use unnest_wider(), we first have to turn our list into a data frame which we did with tibble() above. Next, we pipe the data frame to unnest_wider() and tell it what column we want it to work on. In this case we only have one called data. After we run the code, we something a lot bigger. Actually, the number of rows are the same, but the columns are now equivalent to the number of items that flights$data contains. And each column name corresponds to the name of that item. However, we can see that a lot of the columns still contains lists.\n\ndata_flight |> \n  tidyr::unnest_wider(data) |> \n  head()\n\n# A tibble: 6 × 8\n  flight_d…¹ fligh…² departure    arrival      airline      flight       aircr…³\n  <chr>      <chr>   <list>       <list>       <list>       <list>       <list> \n1 2023-01-19 schedu… <named list> <named list> <named list> <named list> <NULL> \n2 2023-01-19 schedu… <named list> <named list> <named list> <named list> <NULL> \n3 2023-01-18 active  <named list> <named list> <named list> <named list> <NULL> \n4 2023-01-18 active  <named list> <named list> <named list> <named list> <NULL> \n5 2023-01-18 schedu… <named list> <named list> <named list> <named list> <NULL> \n6 2023-01-18 schedu… <named list> <named list> <named list> <named list> <NULL> \n# … with 1 more variable: live <list>, and abbreviated variable names\n#   ¹​flight_date, ²​flight_status, ³​aircraft\n\n\nHowever, we can see that a lot of the columns still contains lists. No problem. We can easily take care of that with another call to unnest_wider(). If we string together two calls to the function, we can get to the information in the lists under departure. Now our data frame is a lot bigger.\n\ndata_flight |> \n  tidyr::unnest_wider(data) |> \n  tidyr::unnest_wider(departure, names_sep = \"_\") |> \n  head()\n\n# A tibble: 6 × 19\n  flight_date flight_s…¹ depar…² depar…³ depar…⁴ depar…⁵ depar…⁶ depar…⁷ depar…⁸\n  <chr>       <chr>      <chr>   <chr>   <chr>   <chr>   <chr>   <chr>     <int>\n1 2023-01-19  scheduled  Hong K… Asia/H… HKG     VHHH    1       <NA>         NA\n2 2023-01-19  scheduled  Hong K… Asia/H… HKG     VHHH    1       <NA>         NA\n3 2023-01-18  active     Ronald… Americ… DCA     KDCA    2       B15          NA\n4 2023-01-18  active     Pittsb… Americ… PIT     KPIT    <NA>    B37          NA\n5 2023-01-18  scheduled  Luis M… Americ… SJU     TJSJ    A       5             3\n6 2023-01-18  scheduled  Charle… Americ… CHS     KCHS    <NA>    B8           34\n# … with 10 more variables: departure_scheduled <chr>,\n#   departure_estimated <chr>, departure_actual <chr>,\n#   departure_estimated_runway <chr>, departure_actual_runway <chr>,\n#   arrival <list>, airline <list>, flight <list>, aircraft <list>,\n#   live <list>, and abbreviated variable names ¹​flight_status,\n#   ²​departure_airport, ³​departure_timezone, ⁴​departure_iata, ⁵​departure_icao,\n#   ⁶​departure_terminal, ⁷​departure_gate, ⁸​departure_delay\n\n\nWe can string together as many calls to unnest_wider() as we need until we get the output that we are looking for. We can remove the columns that end with “codeshared” as we won’t be utilizing that information.\nWe are saving the output as a data frame names tb_flight.\n\ntb_flight <- data_flight |> \n  tidyr::unnest_wider(data) |> \n  tidyr::unnest_wider(departure, names_sep = \"_\") |> \n  tidyr::unnest_wider(arrival, names_sep = \"_\") |> \n  tidyr::unnest_wider(airline, names_sep = \"_\") |> \n  tidyr::unnest_wider(flight, names_sep = \"_\") |> \n  select(-ends_with(c(\"codeshared\")))\n\nHere is what our final data frame looks like.\n\ntb_flight |>\n  head() |> \n  gt()\n\n\n\n\n\n\nflight_date\n      flight_status\n      departure_airport\n      departure_timezone\n      departure_iata\n      departure_icao\n      departure_terminal\n      departure_gate\n      departure_delay\n      departure_scheduled\n      departure_estimated\n      departure_actual\n      departure_estimated_runway\n      departure_actual_runway\n      arrival_airport\n      arrival_timezone\n      arrival_iata\n      arrival_icao\n      arrival_terminal\n      arrival_gate\n      arrival_baggage\n      arrival_delay\n      arrival_scheduled\n      arrival_estimated\n      arrival_actual\n      arrival_estimated_runway\n      arrival_actual_runway\n      airline_name\n      airline_iata\n      airline_icao\n      flight_number\n      flight_iata\n      flight_icao\n      aircraft\n      live\n    \n\n\n2023-01-19\nscheduled\nHong Kong International\nAsia/Hong_Kong\nHKG\nVHHH\n1\nNA\nNA\n2023-01-19T03:20:00+00:00\n2023-01-19T03:20:00+00:00\nNA\nNA\nNA\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n8\nNA\nNA\nNA\n2023-01-19T06:00:00+00:00\n2023-01-19T06:00:00+00:00\nNA\nNA\nNA\nMalaysia Airlines\nMH\nMAS\n9210\nMH9210\nMAS9210\n\n\n\n\n2023-01-19\nscheduled\nHong Kong International\nAsia/Hong_Kong\nHKG\nVHHH\n1\nNA\nNA\n2023-01-19T03:20:00+00:00\n2023-01-19T03:20:00+00:00\nNA\nNA\nNA\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n8\nNA\nNA\nNA\n2023-01-19T06:00:00+00:00\n2023-01-19T06:00:00+00:00\nNA\nNA\nNA\nCathay Pacific\nCX\nCPA\n844\nCX844\nCPA844\n\n\n\n\n2023-01-18\nactive\nRonald Reagan Washington National Airport\nAmerica/New_York\nDCA\nKDCA\n2\nB15\nNA\n2023-01-18T06:10:00+00:00\n2023-01-18T06:10:00+00:00\nNA\nNA\nNA\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n4\nB55\nNA\nNA\n2023-01-18T07:37:00+00:00\n2023-01-18T07:37:00+00:00\nNA\nNA\nNA\nAlitalia\nAZ\nAZA\n5849\nAZ5849\nAZA5849\n\n\n\n\n2023-01-18\nactive\nPittsburgh International\nAmerica/New_York\nPIT\nKPIT\nNA\nB37\nNA\n2023-01-18T05:50:00+00:00\n2023-01-18T05:50:00+00:00\nNA\nNA\nNA\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n8\n37\nNA\nNA\n2023-01-18T07:30:00+00:00\n2023-01-18T07:30:00+00:00\nNA\nNA\nNA\nQatar Airways\nQR\nQTR\n7678\nQR7678\nQTR7678\n\n\n\n\n2023-01-18\nscheduled\nLuis Munoz Marin International\nAmerica/Puerto_Rico\nSJU\nTJSJ\nA\n5\n3\n2023-01-18T06:20:00+00:00\n2023-01-18T06:20:00+00:00\n2023-01-18T06:22:00+00:00\n2023-01-18T06:22:00+00:00\n2023-01-18T06:22:00+00:00\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n5\n11\n3\nNA\n2023-01-18T09:25:00+00:00\n2023-01-18T09:25:00+00:00\nNA\nNA\nNA\nJetBlue Airways\nB6\nJBU\n404\nB6404\nJBU404\n\n\n\n\n2023-01-18\nscheduled\nCharleston, AFB Municipal\nAmerica/New_York\nCHS\nKCHS\nNA\nB8\n34\n2023-01-18T10:45:00+00:00\n2023-01-18T10:45:00+00:00\nNA\nNA\nNA\nJohn F Kennedy International\nAmerica/New_York\nJFK\nKJFK\n5\n23\n5\nNA\n2023-01-18T12:35:00+00:00\n2023-01-18T12:35:00+00:00\nNA\nNA\nNA\nTAP Air Portugal\nTP\nTAP\n4266\nTP4266\nTAP4266\n\n\n\n\n\n\n\n\n\nLooking at our data, there’s something that stands out, or should I say doesn’t stand out. This data doesn’t have any geographic information! We can’t plot anything with this data.\nWell, thankfully for us, there is a nifty package that can help us called airportr. Airportr is a pretty simple package that does exactly what we need it to do. It uses data from openfligts.org to take an airport name, such as an IATA code, and produce a bunch of information about that airport, including it’s latitude and longitude! We can plot now! WOOHOO!\nSo let’s get down to business. We are going to take our unnested data frame and use the IATA codes to get the latitude and longitude of each airport currently with flights heading to JFK airport.\nTo do this, we are going to use purrr::map() which takes a list or vector of values, and applies them one at a time to a function. The function we are applying them to is airportr::airport_detail() which is going to return the information we need.\npurrr::map() returns a list of values however and we want a data frame, so purrr::list_rbind() is going to collapse that list into exactly what we want. Once we have a data frame, we can add back in all the flight information from the API call to include as details in our map.\n\nlat_lon_dep <- tb_flight |> \n  mutate(departure_country = str_split(departure_timezone, '/')) |> \n  pull(departure_iata) |> \n  map(\\(x) airportr::airport_detail(input = x, input_type = \"IATA\")) |> \n  setNames(tb_flight$departure_iata) |> \n  list_rbind() |> \n  bind_cols(tb_flight |> \n              select(ends_with(\"scheduled\"), \n                     airline_name, \n                     flight_number)) |>\n  mutate(departure_scheduled = as.character(ymd_hms(departure_scheduled)),\n         arrival_scheduled = as.character(ymd_hms(arrival_scheduled)))\n\nThis bit of code is to get us the location of JFK airport since flights don’t leave JFK to arrive at JFK, or do they? That would be weird.\n\narr_location <- map_vec(\"JFK\", \\(x) airportr::airport_location(input = x, input_type = \"IATA\"))\n\narr_location\n\n# A tibble: 1 × 2\n  Latitude Longitude\n     <dbl>     <dbl>\n1     40.6     -73.8\n\n\nAnyway, once we get JFK’s lat and long, we can do something to help us with our plotting later. On our map, we want to have a line point from the departure airport to JFK. To do this we need to have the a special data type called a LINESTRING which we well be creating later, but this is a necessary step to creating it.\nWe group the data frame we created by IATA code and then we pass the grouped data frame to a function called group_modify(). This function lets us apply function to each group, and we are using it to get one row for each IATA code found in our data. Then, for each group we are adding the longitude and latitude for JFK.\n\nlat_lon_line <- lat_lon_dep |>\n  group_by(IATA) |> \n  group_modify(~ head(.x, 1L)) |> \n  group_modify(~ add_row(.x, \n                         Latitude = arr_location$Latitude, \n                         Longitude = arr_location$Longitude))\n\nThis is what our output will look like.\nAs you can see, the second occurance of each IATA code contains the latitude and longitude of JFK.\n\nlat_lon_line |> \n  select(IATA, Longitude, Latitude) |> \n  head()\n\n# A tibble: 6 × 3\n# Groups:   IATA [3]\n  IATA  Longitude Latitude\n  <chr>     <dbl>    <dbl>\n1 AMS        4.76     52.3\n2 AMS      -73.8      40.6\n3 ATL      -84.4      33.6\n4 ATL      -73.8      40.6\n5 BGR      -68.8      44.8\n6 BGR      -73.8      40.6\n\n\nWhen working with geographical data in R, the sf package going to be your best friend. This package contains so many functions to work with and map out geographic data. And most importantly for us, turn geographic data in numeric form into a geographic data structure called a geometry.\n\nlibrary(sf)\n\nsf::st_as_sf() is going to turn our long and lat values into a geometry data structure. All we have to do is give it our data frame, the column names that correspond to our long and lat values, and a value for our coordinate reference system. 4326 is a standard CRS code that helps applies the same mapping projection to all of our long and lat values when they are plotted.\n\ndep_sf <- sf::st_as_sf(lat_lon_line, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nHere’s why we went through all that trouble with group_modify() before. When we group dep_sf by IATA code and then call summarise() on it, we get the latitude and longitude for JFK and the other airport in the same column.\n\ndep_sf |> \n  group_by(IATA) |> \n  summarise() |> \n  head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -84.4281 ymin: 33.6367 xmax: 4.76389 ymax: 52.3086\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 2\n  IATA                                  geometry\n  <chr>                         <MULTIPOINT [°]>\n1 AMS    ((4.76389 52.3086), (-73.7789 40.6398))\n2 ATL   ((-84.4281 33.6367), (-73.7789 40.6398))\n3 BGR   ((-68.8281 44.8074), (-73.7789 40.6398))\n4 BOS   ((-73.7789 40.6398), (-71.0052 42.3643))\n5 BTV   ((-73.1533 44.4719), (-73.7789 40.6398))\n6 BUF   ((-73.7789 40.6398), (-78.7322 42.9405))\n\n\nNext, when we call another sf function, st_cast(), with the argument “LINESTRING”, we get the geometry that we have been after. Now, we can plot lines between the airports!\n\ndep_sf |> \n  group_by(IATA) |> \n  summarise() |> \n  st_cast(\"LINESTRING\") |> \n  head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -84.4281 ymin: 33.6367 xmax: 4.76389 ymax: 52.3086\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 2\n  IATA                              geometry\n  <chr>                     <LINESTRING [°]>\n1 AMS    (4.76389 52.3086, -73.7789 40.6398)\n2 ATL   (-84.4281 33.6367, -73.7789 40.6398)\n3 BGR   (-68.8281 44.8074, -73.7789 40.6398)\n4 BOS   (-73.7789 40.6398, -71.0052 42.3643)\n5 BTV   (-73.1533 44.4719, -73.7789 40.6398)\n6 BUF   (-73.7789 40.6398, -78.7322 42.9405)\n\n\nThe final part of the pipe is adding on the information from our API call and the airportr function call.\n\ndep_line <- dep_sf |> \n  group_by(IATA) |> \n  summarise() |> \n  st_cast(\"LINESTRING\") |> \n  bind_cols(lat_lon_dep |>\n              group_by(IATA) |> \n              group_modify(~ head(.x, 1L)) |> \n              ungroup() |> \n              select(-c(IATA))\n            )\n\n\nWe have finally made it to plotting our data! Wow, that was a long road but I am glad we are here.\nWe are going to be using the leaflet R package which is the R implementation of a popular open-source JavaScript library for interactive mapping.\nI highly recommend checking out the documentation for using leaflet because it is very powerful, and what I am about to show you is a pretty simple use case in comparison to what is out there.\n\nlibrary(leaflet)\n\nAll leaflet code begins with the leaflet() function. Then we add our base map with addTiles(). Now we add our geometries. addPolylines() is how we add our LINESTIRNG data. We don’t have to specify our columns in this instance because leaflet is looking for the geometry data structure we made. Next, we are adding markers for the airport locations using addMarkers(). In this case, we are referring to column names because we don’t have a geometry data structure in this object. I’m also adding labels so information about each airport and the flight shows up when we hover over a marker, and finally, we are clustering markers because some airports have multiple flights going to JFK throughout the day.\n\nLabelslabel <- sprintf(\n  \"<strong>%s</strong><br/>%s<br/>%s<br/>Flight Number <strong>%s</strong>\n  <br/>Departure Time %s<br/> Arrvial Time %s\",\n  lat_lon_dep$City, lat_lon_dep$Name, lat_lon_dep$airline_name,\n  lat_lon_dep$flight_number, lat_lon_dep$departure_scheduled, lat_lon_dep$arrival_scheduled\n) %>% lapply(htmltools::HTML)\n\n\n\nm <- \n  leaflet() |> \n  addTiles() |> \n  leaflet::addPolylines(data = dep_line) |> \n  addMarkers(data = lat_lon_dep, \n             lng = ~Longitude, \n             lat = ~Latitude, \n             label = label,\n             clusterOptions = markerClusterOptions(freezeAtZoom = 5))\n\nSo with all our code put together, here’s our finished map.\n\nm\n\n\n\n\n\n\nI thought that this code could probably be put into a function, so I did!\nIf you want to try this out for yourself, here’s all the code you need. Don’t forget to install the necessary libraries!\nAnd for a bonus, at the bottom of this page there is a map using this function to map flights going to San Francisco!\n\nflights_map <- function(api, arr_IATA) {\n  \n  r <- httr::GET(\"http://api.aviationstack.com/v1/flights\", \n                 query = list(access_key = api, arr_IATA = arr_IATA))\n  \n  r\n  \n  httr::status_code(r)\n  \n  flights <- httr::content(r)\n  \n  data_flight <- tibble(flights$data)\n  \n  tb_flight <- data_flight |> \n    tidyr::unnest_wider(col = c(\"flights$data\")) |> \n    tidyr::unnest_wider(col = c(departure), names_sep = \"_\") |> \n    tidyr::unnest_wider(col = c(arrival), names_sep = \"_\") |> \n    tidyr::unnest_wider(col = c(airline), names_sep = \"_\") |> \n    tidyr::unnest_wider(col = c(flight), names_sep = \"_\") |> \n    dplyr::select(-ends_with(c(\"codeshared\")))\n  \n  lat_lon_dep <- tb_flight |> \n    dplyr::mutate(departure_country = str_split(departure_timezone, '/')) |> \n    dplyr::pull(departure_iata) |> \n    purrr::map(\\(x) airportr::airport_detail(input = x, \n                                             input_type = \"IATA\")) |> \n    setNames(tb_flight$departure_iata) |> \n    purrr::list_rbind() |> \n    dplyr::bind_cols(tb_flight |> \n                       dplyr::select(ends_with(\"scheduled\"), \n                                     airline_name, \n                                     flight_number)) |>\n    dplyr::mutate(departure_scheduled = as.character(ymd_hms(departure_scheduled)),\n                  arrival_scheduled = as.character(ymd_hms(arrival_scheduled)))\n  \n  arr_location <- purrr::map_vec(arr_iata, \\(x) airportr::airport_location(input = x, \n                                                                           input_type = \"IATA\"))\n  \n  lat_lon_line <- lat_lon_dep |>\n    dplyr::group_by(IATA) |> \n    dplyr::group_modify(~ head(.x, 1L)) |> \n    dplyr::group_modify(~ add_row(.x, \n                                  Latitude = arr_location$Latitude, \n                                  Longitude = arr_location$Longitude))\n  \n  \n  dep_sf <- sf::st_as_sf(lat_lon_line, \n                         coords = c(\"Longitude\", \"Latitude\"), \n                         crs = 4326)\n  \n  \n  dep_line <- dep_sf |> \n    dplyr::group_by(IATA) |> \n    dplyr::summarise() |> \n    sf::st_cast(\"LINESTRING\") |> \n    dplyr::bind_cols(lat_lon_dep |>\n                       dplyr::group_by(IATA) |> \n                       dplyr::group_modify(~ head(.x, 1L)) |> \n                       dplyr::ungroup() |> \n                       dplyr::select(-c(IATA))\n    )\n  \n  label <- sprintf(\n    \"<strong>%s</strong><br/>%s<br/>%s<br/>Flight Number <strong>%s</strong>\n  <br/>Departure Time %s<br/> Arrvial Time %s\",\n  lat_lon_dep$City, \n  lat_lon_dep$Name, \n  lat_lon_dep$airline_name,\n  lat_lon_dep$flight_number, \n  lat_lon_dep$departure_scheduled,\n  lat_lon_dep$arrival_scheduled\n  ) %>% \n    lapply(htmltools::HTML)\n  \n  m <- \n    leaflet::leaflet() |> \n    leaflet::addTiles() |> \n    leaflet::addPolylines(data = dep_line) |> \n    leaflet::addMarkers(data = lat_lon_dep, \n                        lng = ~Longitude, \n                        lat = ~Latitude, \n                        label = label,\n                        clusterOptions = leaflet::markerClusterOptions(freezeAtZoom = 5))\n  m\n}\n\napi <- rstudioapi::askForPassword()\n\nflights_map(api, \"SFO\")\n\n\n\n\n\nm"
  }
]