[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sam Weiner is an aspiring Data Scientist who is excited to bring creativity and analytical know-how to your data science team. I am a current masters student at the Institute for Advanced Analytics, where I am studying data science and technical communication to be able to solve complex problems and share results answers with a wide audience.\nI became interested in data science because of my growing curiosity. In my undergraduate philosophy courses, I enjoyed learning about and debating the answers to fascinating philosophical questions, but as my interests broadened from theoretical questions to practical ones, I realized that the skills I had as a philosopher were insufficient to answering real-word questions. When I heard about data science for the first time, I knew that becoming a data scientist was going to be my path forward to gaining a better understanding of our world. My curiosity has only grown as I have embarked on this journey, and I am so excited to dive into all of the data I can get my hands on to deepen my knowledge of the relationships and events that make up our endlessly fascinating world!\nMy journey has led me to become proficient in R, Python, and SQL. I have become intimately familiar with statistical learning techniques in the inferential world - linear regression, logistic regression, decision trees, causal inference - as well as in the predictive world - random forests, XGBoost, neural networks. I am leveraging Tableau and ggplot2 to create intuitive data visualizations to communicate data analysis to a broad audience.\nI am still passionate about big philosophical questions, and I love to have stimulating conversations with my friends or strangers! In my free time, you might see me watching college wrestling (Go RU!) or obsessing over the perfect hummus recipe."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nRutgers University | New Brunswick, NJ\nB.A. in Philosophy | Sept 2018 - May 2021\nInstitute for Advanced Analytics, North Carolina State University | Raleigh, NC\nM.S. Candidate in Analytics | June 2022 - Current"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dialectic with Data",
    "section": "",
    "text": "Tidy Tuesday - 12/20\n\n\n\n\n\n\n\ntidytuesday\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSam Weiner\n\n\n\n\n\n\n  \n\n\n\n\nR resources for Practicum\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nSam Weiner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday - World Cup Edition\n\n\n\n\n\n\n\ntidytuesday\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nSam Weiner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSam Weiner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/R-resources/index.html",
    "href": "posts/R-resources/index.html",
    "title": "R resources for Practicum",
    "section": "",
    "text": "Hello again! I’m excited to put another post up on Dialectic with Data where I will be brain dumping some of the awesome R resources I have ran across in my journey learning and loving R! This post is was specifically inspired by my practicum team because I have enjoyed sharing these resources with them over time, but many of them have been lost deep in the depths of our slack channel. So, I am going to be cataloging many of the things that I have shared with them, and I will also be adding new resources as I come upon them for them and you!"
  },
  {
    "objectID": "posts/R-resources/index.html#general-r-tips",
    "href": "posts/R-resources/index.html#general-r-tips",
    "title": "R resources for Practicum",
    "section": "\n1 General R tips",
    "text": "1 General R tips\n\n1.1 R for Data Science\nMy biggest and most valuable tip to learning R is read R for Data Science by Hadley Wickham and Garret Grolemund.This book has so much knowledge stored inside it. It primarily teaches the tidyverse and its wonderful array of tools for exploratory data analysis and data visualization. It has been a bible for me, and I see myself turning back to it more often than one would think for a self-classified introductory book. The second version is currently under development but available to read here.\n\n1.2 Rstudio Projects\nRstudio projects are a convenient and efficient way of organizing all of the code, data, and visualizations that make up a data science project. The biggest efficiency boost in my experience is that your working directory automatically updates to the folder where your Rstudio project lives, so all of you code and visualizations are automatically saved in the same place. Additionally, any data that you have in, for example, a csv file can be easily uploaded to your environment without having to mess with the working directory. Other important features that are a bit more advanced revolve around Rstudio projects ability to improve reproducibility and use version control.\n\nResources for learning more about Rstudio Projects\n\nIntro to Rstudio Projects\nRstudio and reproducibility\nRstudio and git\n\n\n\n1.3 Shortcuts, Tips, and Tricks\nThe Rstudio IDE is probably the best IDE (integrated development environment) for using R and conducting data science tasks in general. It has stiff competition from the likes of Visual Studio Code, but there are a few tips and tricks that I am going to show you for Rstudio that might make you think twice before switching to a different IDE.\n\n\nAlt + -\n\nShortcut for inserting the assignment operator <-\n\n\n\n\nCtrl + Shift + M\n\nShortcut for inserting the pipe magrittr operator %>% commonly used in the tidyverse\nNote: As of R 4.1.0, R has a built in pipe operator |> which works similarly to the magrittr pipe but has same slight nuances. See here and here for more information on the built in pipe operator.\n\n\n\n\nCtrl + Shift + R\n\nCreates a comment section header in an R source file which is collapsible and also creates an outline to easily navigate your code file.\n\n\n\nCtrl + Shift + Alt + M\n\nRenames a variable in the entire file. Just highlight the variable to be renamed, use the shortcut, and type the new name. Easy as that!\nUsing Ctrl + F and using replace is another option which is useful when you don’t want to rename every instance of a variable in the file.\n\n\nCode Snippets\n\nCode Snippets are ways to insert commonly used code structures (or headers) in an R file. Under the Tools toolbar, go to Edit Code Snippets. There you will see all the code snippets pre-built in Rstudio and and editor for adding new ones. I use this to add a header structure to new scripts where I can fill in information about the script for my teammates to read when they use it.\n\n\nMultiple Cursors\n\nThis was a tip that I was sorely missing from my time using VS Code until I stumbled upon this simple shortcut in Rstudio! Just by holding down the Alt button and clicking or draging with the mouse, you’ll have more cursors than you know what to do with. More awesome shortcuts can be found in this awesome article by Appsilon\n\n\n\n\n1.4 Quarto\nQuarto is Rstudio’s (now called Posit) new Rmarkdown replacement. The biggest change from Rmarkdown to Quarto is that you can use Quarto in Jupyter Notebooks! Posit (formerly Rstudio) is broadening it’s presence in the data science community by engaging with python users in addition to R users. The hope, says Posit, is to create easy tools for cross-language collaboration so that researchers and data scientists who have different languages of choice can work together with ease. From what I have seen, many data scientists who might have preferred R as a language have been pressured to use python because of its large presence in the data science community. This tool is hopefully going to create a pathway for R users to stay R users without complicating team workflows in a python-dominant environment. In addition, Quarto has some awesome features such as helping users easily create documents, presentation, or even blogs (like this one!). So far, it has been very intuitive and the documentation has been very helpful. You can find information at quarto.org."
  },
  {
    "objectID": "posts/R-resources/index.html#packages",
    "href": "posts/R-resources/index.html#packages",
    "title": "R resources for Practicum",
    "section": "\n2 Packages",
    "text": "2 Packages\nNow lets get to packages! I am going to be listing packages that I think are really helpful (and really cool) for working with data and doing modeling in R.\n\n2.1 Tidyverse\n\n\nggplot2\n\nThis package contains the best tools for taking data and making a stellar visualization out of it. It is extremely versatile and at times very complicated. Later on I will be sharing more about ggplot2 and provide some resources for making ggplot2 super easy to understand.\n\n\n\ndplyr\n\nThis package has the tools you need to manipulate data. Mutate is useful for feature engineering and summarise is great for calculating summaries of your data set. Using the piping syntax makes this package so powerful. My favorite way of using this package is to dig into the data and pipe what I want into a ggplot!\n\n\n\nreadr\n\nThis package makes importing data easy as can be.\n\n\n\npurrr\n\nThis package contains the tidyverse’s toolkit for functional programming. The iteration chapter in R for Data Science is a great place to start if you are new to functional programming. Another great resources I used is purrr tutorial by Jenny Bryan. Functional programming has been very hard for me to learn, but these two resources have really helped me wrap my brain around the concept, and it has already impacted my projects enormously.\n\n\n\nstringr\n\nstringr is the handiest way of working with strings in R. It has everything under the sun when it comes to manipulating and cleaning character data.\n\n\n\nforcats\n\nFactors are a very common data type in R, but they can be tricky at times. This package has some awesome tools for working with factors that will really come in handy if you use factors a lot.\n\n\n\n2.2 Tidymodels\nTidymodels is a standalone universe which has everything you need to do machine learning in R. The tidymodels project is headed by the same person that create CARET years back, Max Kuhn, and the goal of tidymodels is the same as CARETS goal: To create a unified interface for machine learning in R. The package makes use of R’s rich ecosystem of machine learning model packages but standardizes the interface across all those implementations to make the switch between models seamless. The amount of packages and tools in the tidymodels universe is large and there is too much to say for this blog post. However, I do plan on showing how tidymodels is used in a future blog post, so stay tuned! In the meantime, I am going to point you to some amazing resources if you want to get started now. - Tidy Modeling with R is a free book written by Max Kuhn and Julia Silge which is the best resource for getting started with tidymodels and it also has some great information on machine learning in general! - tidymodels.org has some amazing content which will be the preferred resource for people who want a shorter format introduction to tidymodels. - juilasilge.com is the blog of one of the co-authors of Tidy Modeling with R and she has some great blog posts and youtube videos where she implements the tidymodels packages and gives great explanations of what she is doing.\n\n2.3 Awesome Packages\nThe Awesome Packages section is going to be a growing list of packages that I find really cool because they have either changed and/or improved the way I use R as a data science student.\n\n2.3.1 broom\nbroom is a super cool R package. One of the most annoying aspects of using R for data science is trying to extract the output from the model object. Summaries of the model object are easy enough to get using summary(), but to extract the components of model like the coefficients, pvalues, or Rsquared value to use for other operations means digging into the model object and trying and failing to use the right index to find it. broom makes that task no more. By using the tidy() function on supported model objects, the output is represented as a data frame which makes extracting what you want so much easier. Rsquared and other metric vaules can be found by using glance() and augment() can add output like predictions from a new dataset back into the data set from whence it came.\n\nlibrary(tidyverse)\nlibrary(broom)\nmpg_lm <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n\n\n\nsummary()\ntidy()\nglance()\naugment()\n\n\n\n\nsummary(mpg_lm) # Normal summary output\n\n\nCall:\nlm(formula = mpg ~ wt + hp + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\nwt          -3.16697    0.74058  -4.276 0.000199 ***\nhp          -0.01804    0.01188  -1.519 0.140015    \ncyl         -0.94162    0.55092  -1.709 0.098480 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n\n\n\n\n\ntidy(mpg_lm) # tidy() returns coefficients and pvalues\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  38.8       1.79       21.7  4.80e-19\n2 wt           -3.17      0.741      -4.28 1.99e- 4\n3 hp           -0.0180    0.0119     -1.52 1.40e- 1\n4 cyl          -0.942     0.551      -1.71 9.85e- 2\n\n\n\n\n\nglance(mpg_lm) # glance returns model metrics\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.843        0.826  2.51    50.2 2.18e-11     3  -72.7  155.  163.    177.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\n\n\n# augment with 'newdata = ' returns entire dataset plus fitted and residual values\naugment(mpg_lm, newdata = mtcars)\n\n# A tibble: 32 × 14\n   .rownames     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with 22 more rows, and 2 more variables: .fitted <dbl>, .resid <dbl>\n\n# augment without newdata uses training data and returns more metrics\naugment(mpg_lm)\n\n# A tibble: 32 × 11\n   .rowna…¹   mpg    wt    hp   cyl .fitted .resid   .hat .sigma .cooksd .std.…²\n   <chr>    <dbl> <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n 1 Mazda R…  21    2.62   110     6    22.8 -1.82  0.0750   2.53 1.15e-2  -0.754\n 2 Mazda R…  21    2.88   110     6    22.0 -1.01  0.0582   2.55 2.67e-3  -0.416\n 3 Datsun …  22.8  2.32    93     4    26.0 -3.16  0.0856   2.48 4.05e-2  -1.32 \n 4 Hornet …  21.4  3.22   110     6    20.9  0.464 0.0533   2.56 5.08e-4   0.190\n 5 Hornet …  18.7  3.44   175     8    17.2  1.53  0.110    2.54 1.29e-2   0.647\n 6 Valiant   18.1  3.46   105     6    20.3 -2.15  0.0699   2.52 1.48e-2  -0.888\n 7 Duster …  14.3  3.57   245     8    15.5 -1.19  0.118    2.55 8.53e-3  -0.506\n 8 Merc 24…  24.4  3.19    62     4    23.8  0.636 0.157    2.55 3.55e-3   0.276\n 9 Merc 230  22.8  3.15    95     4    23.3 -0.496 0.152    2.56 2.06e-3  -0.214\n10 Merc 280  19.2  3.44   123     6    20.0 -0.789 0.0469   2.55 1.27e-3  -0.322\n# … with 22 more rows, and abbreviated variable names ¹​.rownames, ²​.std.resid\n\n\n\n\n\n\n2.3.2 janitor\njanitor is a key package because it does something that, while we might think is important, is not all that interesting, but is a such a quality of life boost for a data scientist. Cleaning dirty data is numerous ways. Cleaning dirty data is a necessary part of the process to extracting value out a data set, but it is such a time sink. Janitor comes in to provide a myriad of helpful functions to get data people working in the data instead of on the data.\n\nlibrary(janitor)\n\n# Data frame with bad column names\ntest <- tribble(\n  ~`Customer ID`, ~RegionCode, ~ElePhanTIntHeRoOm,\n  242, \"A\", 24,\n  3422, \"B\", 353\n)\n\ntest\n\n# A tibble: 2 × 3\n  `Customer ID` RegionCode ElePhanTIntHeRoOm\n          <dbl> <chr>                  <dbl>\n1           242 A                         24\n2          3422 B                        353\n\n# clean_names() takes care of most bad column names in most forms\n# Not always going to work but its great for 99% of the data files you'll encounter\njanitor::clean_names(test)\n\n# A tibble: 2 × 3\n  customer_id region_code ele_phan_t_int_he_ro_om\n        <dbl> <chr>                         <dbl>\n1         242 A                                24\n2        3422 B                               353\n\n\n\n2.3.3 skimr\nskimr is your best friend when beginning exploratory data analysis. Using the skim() function on a data frame, a really easy to understand output will be generated containing summary statistics for all of your columns. The information will include variable type, percent missing, 0th, 25th, 50th, 75th, 100th percentile value for continuous variables, mean, standard deviation, and more! I think that you will love this tool once you start to use it!\n\nlibrary(skimr)\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\n\n2.3.4 reprex\nWe all know what it’s like to feel completely baffled when writing code. At least I can say I have felt completely baffled when writing code and I have felt stuck countless times. And when I am baffled, I go to the internet to find the cure. If I ever have to ask a question on sites like stack overflow or github to ask for help, I always use reprex to help me create a minimally reproducible example. A minimally reproducible example is simple piece of code that replicates your problem so that other knowledgeable people can help you diagnose your issue. Asking questions with reprex gives you the best chance to finding an answer to your question because you have helped the internet help you by giving them the exact situation you are facing.\nCreate a reproducible example of the error that you are encountering. For instance, lets say you are trying to create three boxplots looking at the distribution of some continuous variable across the values of a categorical variable, but the output doesn’t look right. Let’s use the mtcars dataset to illustrate this issue.\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nOnce we have recreated our issue in a simple and easy to follow example, we highlight all of the code that was used to create the example, including the libraries. Then, we run the reprex() function which will nicely format our code with any errors or warnings that we may have gotten and copy that to our clipboard.\n\nlibrary(reprex)\n\nreprex()\n\nThe output will look like this when we upload our code to stackoverflow or github:\n\n\nIt even includes the plot as image which will show up when we post to stack overflow or github!\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n#> Warning: Continuous x aesthetic\n#> ℹ did you forget `aes(group = ...)`?\n\nCreated on 2023-01-08 with reprex v2.0.2\nAlso, by adding factor() around cyl, we can get the plot we were looking for!\n\nggplot(mtcars, aes(factor(cyl), mpg)) +\n  geom_boxplot()\n\n\n\nCorrect way to create boxplot\n\n\n\n\n\n2.3.5 here\nSharing code is an important part of collaborative data science projects which is common in the field. To be able make sure code is able to be run by others when shared, coders must be cognizant of not hard-coding file paths into their scripts. Other people will 99.99% of the time not have the same file structure as you do, and hard coding a file path will make that script only usable by you until all the file paths are changed by the person you shared it with. We don’t want to make sharing code difficult, so I am hoping that you use the here package to help you create relative file paths! The here package makes creating relative file paths as simple as can be and using it will make sharing code will be hindrance free.\n\nlibrary(here)\n\nhere() starts at C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\n\n\nCalling here() will output the top of the project directory\n\nhere()\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\"\n\n\nBy adding folder names in quotation marks, we can easily dive deeper into our project directory.\n\nhere(\"posts\")\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts\"\n\nhere(\"posts\", \"R-resources\")\n\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts/R-resources\"\n\n\n\n2.3.6 todor\nWorking on big projects means that there are always multiple tasks that are being managed at once. The todor package lets you create searchable comments within your R scripts or even entire projects to help you keep track of your tasks. Never forget again with todor!\nUsing todor is as easy writing a comment in R and using a keyword like TODO at the beginning.\n\nlibrary(todor)\n\n# TODO Show an example of todor\n\n<!-- TODO Change this section. -->\nWhen we call the todor() function, we get output like this. As you might have noticed, todor can detect HTML comments as well!\n\n\ntodor\n\n\n\n2.3.7 styler\nSometimes when we’re coding, readability is not on the front of our mind. Use the styler package to help make your scripts more readable so that you know what it’s doing the next time you come back to it. All you have to do is run one function, and the package non-invasively stylizes your code to be in a standard format so that your code looks consistent across projects.\nLet’s say we have code like this. Quite difficult to read through, no?\niris |> \n  filter(Sepal.Length > 5.1) |> select(Petal.Width, Petal.Length) |> group_by(Petal.Length)|> summarise(mean = Petal.Width)\nAfter running styler::style_active_file(), our code is reformmated and much more readable in my opinion.\niris |>\n  filter(Sepal.Length > 5.1) |>\n  select(Petal.Width, Petal.Length) |>\n  group_by(Petal.Length) |>\n  summarise(mean = Petal.Width)"
  },
  {
    "objectID": "posts/R-resources/index.html#ggplot2",
    "href": "posts/R-resources/index.html#ggplot2",
    "title": "R resources for Practicum",
    "section": "\n3 ggplot2",
    "text": "3 ggplot2\nggplot2 is the primary visualization package in R. It allows for extreme creativity when turning data into visualizations. Checkout the #tidytuesday hashtag on twitter to see some of the crazy impressive visualizations made with ggplot2! ggplot2 is not a package that is simple to master, however. For those beginning their journey with ggplot2, I recommend R for Data Science’s chapter on data visualization which gives a great explanation behind the design of ggplot2 and how it is meant to be used. If you need a reference for ggplot2 code, R Graphics Cookbook is a great resource as are the R Graph Gallery and R charts. I find that sometimes I need to understand how ggplot2 takes data and makes visuals out of it, which is what R for Data Science is great for, and other times I just need a resource to show me all the cool ways I can represent my data, which is what those other three resources are there for.\n\n3.1 Extensions\nOne major benefit of ggplot2 being the default visualization package in R is that it has been building blocks of choice for so many R programmers create new visualization capabilities in R and ggplot2. There are hundreds of extension packages for ggplot2 that add onto its functionality in incredible ways. For instance, gganimate creates the ability for you to have animated graphics in R! And the best part is that these extensions are usually very easy to learn because they all use the same ggplot2 mechanics as I was saying earlier. A full list of registered extensions can be found here. One really exquisite ggplot extension I would like to highlight is esquisse. This package creates a tableau-like interface inside of Rstudio, so that we can interactively plot our data! It has been a game changer for me when doing EDA.\n\n\nesquisse example\n\n\n\n3.2 Fonts\nOne way that people like to modify their visualizations from base ggplot2 is by modifying text fonts. Packages like extrafont and showtext make doing this extremely easy. hrbrthemes is an example of pre-built themes that R users have created that take try to enhance ggplot2 output in this way.\nHere’s what we can do with showtext.\nFirst lets look at standard ggplot2 output.\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\nUsing showtext, we can import fonts from google fonts with a simple function and then use some ggplot2 options to include theme in our plot.\n\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Dancing Script\", \"dance\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Covered By Your Grace\", \"grace\")\nfont_add_google(\"Rock Salt\", \"rock\")\n\nshowtext_auto()\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  xlab(\"Sepal Length\") +\n  ylab(\"Sepal Width\") +\n  ggtitle(\"Sepal Width by Sepal Length Colored by Species\") +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(family = \"gochi\", size = 20), \n    axis.title.y = element_text(family = \"bell\", size = 20), \n    plot.title = element_text(family = \"grace\", size = 20), \n    legend.text = element_text(family = \"rock\", size = 20), \n    legend.title = element_text(\"dance\", size = 20)\n    )\n\n\n\nshowtext_end()\n\nWe used four different fonts in one ggplot! I’ll admit it’s not the nicest looking plot I’ve ever made, but the point stands that you can customize ggplots to your hearts content using packages like showtext.\n\n3.3 Scales\nScaling data in a visualization is a crucial part of effective data communication. If the scale of the data is not clear, then any inferences from that communication will either be unclear or misled. The scales package provides some awesome tooling to make very clear and descriptive plot scales.\nHere’s a bad example but an example nonetheless. Using two scales functions, label_dollar() and label_percent, we changed the axis labels of the previous graph to be more descriptive. There are many more powerful functions in the scales package that I am sure will benefit your data communication tasks.\n\nlibrary(scales)\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/R-resources/index.html#databases",
    "href": "posts/R-resources/index.html#databases",
    "title": "R resources for Practicum",
    "section": "\n4 Databases",
    "text": "4 Databases\nWorking with databases in R is not terribly complicated. The DBI package is very useful when connecting to and querying databases. But there are some really cool tools for working with databases in that I you’re going to like.\n\n4.1 dbplyr\ndbplyr is a tidyverse package that converts dplyr code into SQL queries. Yes, you read that right. I need to note right away, however, that it does not aim to replace SQL in your data science workflow; while it is robust, it is not a complete replacement. With that said, when I have been coding in R for a while and have dplyr code right at the front of mind, it has been great to just whip up some quick code to enable some more EDA with dbplyr. It is a great addition to any R users workflow if they work with databases frequently."
  },
  {
    "objectID": "posts/R-resources/index.html#causal-inference",
    "href": "posts/R-resources/index.html#causal-inference",
    "title": "R resources for Practicum",
    "section": "\n5 Causal Inference",
    "text": "5 Causal Inference\nCausal inference is a topic that I have been learning about a lot lately. As someone who does not have a formal background in statistics or epidemiology, it has been important to me to find resources that teach causal inference in terms that I can understand. Here is a list of resources and R packages that can help you if you are interested in causal inference.\n\nCausal Inference in R is a ongoing book project by Malcolm Barrett, Lucy D’Agostino McGowan, and Travis Gerke. It contains some amazing information at the moment and it will only get better as the authors work on it.\nCausal Inference for The Brave and True by Matheus Facure Alves is a book that has great explanations of causal inference techniques and concepts and everything coding related is written in python for those interested in a python implementation of causal inference!\nCausal Inference: What If? by Miguel A. Hernán and James M. Robins is causal inference bible. It is much heavier than the resources above and the focus is purely on the methodology of causal inference. However R code which follows part 2 of the book chapter by chapter can be found here.\nWorkshop: Causal Inference in R is a video workshop by Malcolm Barrett and Lucy D’Agostino McGowan. The workshop has been updated since the video but no new video has surfaced that I could find. The new workshop can be found on github here with the slides and exercise rmarkdown files. The solutions to the exercises can be found here.\nMatchIt is a package that creates matches based on observational data. It has awesome vignettes on its website that explains matching for causal inference in detail and has many method options to implement for matching.\nggdag is a way to plot directed acyclic dags (DAGs) in R using more a ggplot2-like interface as opposed to daggity.\npropensity helps calculate propensity scores and weights for a wide variety of research questions.propensity is under very early development.\ntipr After fitting your model, you can determine the unmeasured confounder needed to tip your analysis.\nhalfmoon The goal of halfmoon is to cultivate balance in propensity score models."
  },
  {
    "objectID": "posts/tidyt_1220/index.html",
    "href": "posts/tidyt_1220/index.html",
    "title": "Tidy Tuesday - 12/20",
    "section": "",
    "text": "Tidy Tuesday released this data set on December 20th, 2022. It was collected as a part of a data science capstone project where national weather data was acquired to learn which areas of the U.S. struggle with weather prediction and the possible reasons why. Specifically, the project focused on the error in high and low temperature forecasting.\nThe data includes 16 months of daily forecasts and observations from 167 cities, as well as a separate data.frame of information about those cities and some other American cities.\nSo let’s dive into this data and learn something about forecasting weather!\n\nlibrary(tidyverse)\nlibrary(skimr)\n#Tidyverts universe of packages\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(lubridate)\n\n\nweather_forecasts <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/weather_forecasts.csv\")\ncities <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/cities.csv\")\noutlook_meanings <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/outlook_meanings.csv\")\n\nThe weather forecasts data set contains daily high and daily low temperature forecasts 48, 36, 24, and 12 prior to every date in the data set. The data also contains the observed high and low temperature for that date. (Note: some dates have missing data for observed temperature.)\nHere’s a glimpse at the column names, the number of rows, and the first couple of values in the weather forecasts data set.\n\nweather_forecasts |> glimpse()\n\nRows: 651,968\nColumns: 10\n$ date                  <date> 2021-01-30, 2021-01-30, 2021-01-30, 2021-01-30,…\n$ city                  <chr> \"ABILENE\", \"ABILENE\", \"ABILENE\", \"ABILENE\", \"ABI…\n$ state                 <chr> \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", …\n$ high_or_low           <chr> \"high\", \"high\", \"high\", \"high\", \"low\", \"low\", \"l…\n$ forecast_hours_before <dbl> 48, 36, 24, 12, 48, 36, 24, 12, 48, 36, 24, 12, …\n$ observed_temp         <dbl> 70, 70, 70, 70, 42, 42, 42, 42, 29, 29, 29, 29, …\n$ forecast_temp         <dbl> NA, NA, NA, 70, NA, NA, 39, 38, NA, NA, NA, 30, …\n$ observed_precip       <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ forecast_outlook      <chr> NA, NA, NA, \"DUST\", NA, NA, \"DUST\", \"SUNNY\", NA,…\n$ possible_error        <chr> \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", …\n\n\nThe data contains multiple time series, one for each combination of city, state, high_or_low, and forecast_hours_before. I think that looking into how the different forecast times compare in accuracy will be interesting to look at, as well as whether forecast accuracy differs at different times in the year.\nI will be using the tsibble package for this data which is useful when working with time series data. The first of which is that it enables us contain multiple time series in the same data structure, a time series tibble, and this allows us run models or check for certain features on multiple series simultaneously. If you want to learn more about time series analysis and how to use the tidyverts packages, I highly recommend reading Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos. It is a very detailed book and covers a lot of ground in the field of time series.\n\nforecast_tsbl <- weather_forecasts |>\n  mutate(city = factor(city),\n         state = factor(state),\n         high_or_low = factor(high_or_low),\n         forecast_hours_before = factor(forecast_hours_before)) |> \n  as_tsibble(key = c(city, state, high_or_low, forecast_hours_before), index = date)\n\nI am going to be subsetting the data to look at one city, Newark, NJ. This visualization is a time plot of the observed daily high temperature for Newark, NJ from January 30th, 2021 to June 1st, 2022.\n\nforecast_tsbl |> \n  filter(city == \"NEWARK\", high_or_low == \"high\", forecast_hours_before == 12) |>\n  autoplot(observed_temp)\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nSliding Window\nWe will be looking at the rolling mean absolute error between forecasted high and low temperatures versus observed high and low temperatures across the different temperature forecasts.\nI am using a 30 day window around each date to compute the mean. I am also ignoring dates with missing values.\n\nslide_tsbl <- forecast_tsbl |> \n  filter(city == \"NEWARK\") |> \n  mutate(error = forecast_temp - observed_temp) |> \n  mutate(mean_temp_error = slider::slide_dbl(error,\n                                              ~mean(abs(.), na.rm = TRUE), \n                                              .before = 15,\n                                              .after = 15, \n                                              .complete = TRUE))\n\nThis plot shows the mean absolute error across the dates for the high temperature forecast across forecast times.\n\n\nCode\nslide_tsbl |> \n  filter(high_or_low == \"high\") |> \n  ggplot() +\n  geom_line(aes(x = date, y = mean_temp_error)) + \n  facet_grid(vars(high_or_low, forecast_hours_before)) +\n  xlab(\"Date\") +\n  ylab(\"Mean Error\")\n\n\n\n\n\nThis plot shows the average error across the dates for the low temperature forecast across forecast times.\n\n\nCode\nslide_tsbl |> \n  filter(high_or_low == \"low\") |> \n  ggplot() +\n  geom_line(aes(x = date, y = mean_temp_error)) + \n  facet_grid(vars(high_or_low, forecast_hours_before))  +\n  xlab(\"Date\") +\n  ylab(\"Mean Error\")\n\n\n\n\n\nWe can see the overall mean absolute error for each forecast.\n\nforecast_tsbl |> \n  as_tibble() |> \n  filter(city == \"NEWARK\", state == \"NJ\") |>  \n  mutate(error = forecast_temp - observed_temp) |>\n  group_by(forecast_hours_before) |> \n  summarise(MAE = mean(abs(error), na.rm = T))\n\n# A tibble: 4 × 2\n  forecast_hours_before   MAE\n  <fct>                 <dbl>\n1 12                     1.98\n2 24                     2.12\n3 36                     2.29\n4 48                     2.41"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello World!\nI am excited to start this blog where I will share my journey learning data science. I hope to share data analysis that I have conducted using the awesome tool-kit I have gained at the Institute for Advanced Analytics, some R packages that I think are cool, and my thoughts on the field of data science as a whole. Cheers to a new adventure!\n- Sam"
  },
  {
    "objectID": "posts/world-cup/index.html",
    "href": "posts/world-cup/index.html",
    "title": "Tidy Tuesday - World Cup Edition",
    "section": "",
    "text": "Tidy Tuesday is an awesome initiative by the R for Data Science Community where data sets are published every week for the R community share any analysis and visualizations that can be conjured from the data!\n\n\nThis data set contains data on every World Cup since 1930.\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tidymodels)\ntheme_set(theme_light())\n\n\n\n\n\ntt <- tt_load(\"2022-11-29\")\ntt\n\nThis episode of Tidy Tuesday contains two data sets. wcmatches (matches from now on) contains 900 rows, one for each game played on the World Cup stage. It has variables for who played who, what stage (or round) of the tournament the game was played, who won, and the goals scored by each team.\n\nmatches <- tt$wcmatches\nglimpse(matches)\n\nRows: 900\nColumns: 15\n$ year           <dbl> 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1…\n$ country        <chr> \"Uruguay\", \"Uruguay\", \"Uruguay\", \"Uruguay\", \"Uruguay\", …\n$ city           <chr> \"Montevideo\", \"Montevideo\", \"Montevideo\", \"Montevideo\",…\n$ stage          <chr> \"Group 1\", \"Group 4\", \"Group 2\", \"Group 3\", \"Group 1\", …\n$ home_team      <chr> \"France\", \"Belgium\", \"Brazil\", \"Peru\", \"Argentina\", \"Ch…\n$ away_team      <chr> \"Mexico\", \"United States\", \"Yugoslavia\", \"Romania\", \"Fr…\n$ home_score     <dbl> 4, 0, 1, 1, 1, 3, 0, 0, 1, 6, 1, 0, 0, 4, 3, 6, 6, 4, 2…\n$ away_score     <dbl> 1, 3, 2, 3, 0, 0, 4, 3, 0, 3, 0, 1, 4, 0, 1, 1, 1, 2, 3…\n$ outcome        <chr> \"H\", \"A\", \"A\", \"A\", \"H\", \"H\", \"A\", \"A\", \"H\", \"H\", \"H\", …\n$ win_conditions <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ winning_team   <chr> \"France\", \"United States\", \"Yugoslavia\", \"Romania\", \"Ar…\n$ losing_team    <chr> \"Mexico\", \"Belgium\", \"Brazil\", \"Peru\", \"France\", \"Mexic…\n$ date           <date> 1930-07-13, 1930-07-13, 1930-07-14, 1930-07-14, 1930-0…\n$ month          <chr> \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\", \"Jul\",…\n$ dayofweek      <chr> \"Sunday\", \"Sunday\", \"Monday\", \"Monday\", \"Tuesday\", \"Wed…\n\n\nworldcups (wcups from now on) has information about each world cup on a high level. It contains information like the name of the host country each World Cup, the names of the top four finishers, total tournament goals scored, and the number of teams, games, and attendants.\n\nwcups <- tt$worldcups\nglimpse(wcups)\n\nRows: 21\nColumns: 10\n$ year         <dbl> 1930, 1934, 1938, 1950, 1954, 1958, 1962, 1966, 1970, 197…\n$ host         <chr> \"Uruguay\", \"Italy\", \"France\", \"Brazil\", \"Switzerland\", \"S…\n$ winner       <chr> \"Uruguay\", \"Italy\", \"Italy\", \"Uruguay\", \"West Germany\", \"…\n$ second       <chr> \"Argentina\", \"Czechoslovakia\", \"Hungary\", \"Brazil\", \"Hung…\n$ third        <chr> \"USA\", \"Germany\", \"Brazil\", \"Sweden\", \"Austria\", \"France\"…\n$ fourth       <chr> \"Yugoslavia\", \"Austria\", \"Sweden\", \"Spain\", \"Uruguay\", \"W…\n$ goals_scored <dbl> 70, 70, 84, 88, 140, 126, 89, 89, 95, 97, 102, 146, 132, …\n$ teams        <dbl> 13, 16, 15, 13, 16, 16, 16, 16, 16, 16, 16, 24, 24, 24, 2…\n$ games        <dbl> 18, 17, 18, 22, 26, 35, 32, 32, 32, 38, 38, 52, 52, 52, 5…\n$ attendance   <dbl> 434000, 395000, 483000, 1337000, 943000, 868000, 776000, …"
  },
  {
    "objectID": "posts/world-cup/index.html#world-cup-dataset",
    "href": "posts/world-cup/index.html#world-cup-dataset",
    "title": "Tidy Tuesday - World Cup Edition",
    "section": "World Cup dataset",
    "text": "World Cup dataset\nAs was mentioned above, the World Cup dataset contains more general information about each World Cup.\n\nwcups\n\n# A tibble: 21 × 10\n    year host        winner      second third fourth goals…¹ teams games atten…²\n   <dbl> <chr>       <chr>       <chr>  <chr> <chr>    <dbl> <dbl> <dbl>   <dbl>\n 1  1930 Uruguay     Uruguay     Argen… USA   Yugos…      70    13    18  434000\n 2  1934 Italy       Italy       Czech… Germ… Austr…      70    16    17  395000\n 3  1938 France      Italy       Hunga… Braz… Sweden      84    15    18  483000\n 4  1950 Brazil      Uruguay     Brazil Swed… Spain       88    13    22 1337000\n 5  1954 Switzerland West Germa… Hunga… Aust… Urugu…     140    16    26  943000\n 6  1958 Sweden      Brazil      Sweden Fran… West …     126    16    35  868000\n 7  1962 Chile       Brazil      Czech… Chile Yugos…      89    16    32  776000\n 8  1966 England     England     West … Port… Sovie…      89    16    32 1614677\n 9  1970 Mexico      Brazil      Italy  West… Urugu…      95    16    32 1673975\n10  1974 Germany     West Germa… Nethe… Pola… Brazil      97    16    38 1774022\n# … with 11 more rows, and abbreviated variable names ¹​goals_scored,\n#   ²​attendance\n\n\nWe can count the number of total goals scored in a World Cup and rank them by which tournament had the most goals scored.\n\nwcups %>% \n  count(year, wt = goals_scored, sort = TRUE)\n\n# A tibble: 21 × 2\n    year     n\n   <dbl> <dbl>\n 1  1998   171\n 2  2014   171\n 3  2018   169\n 4  2002   161\n 5  2006   147\n 6  1982   146\n 7  2010   145\n 8  1994   141\n 9  1954   140\n10  1986   132\n# … with 11 more rows\n\n\nLet’s also visualize the relationship between number of goals scored and the number of matches played in each World Cup.\n\nwcups %>% \n  ggplot(aes(year, goals_scored)) +\n  geom_line() +\n  geom_line(aes(year, games))"
  }
]