{
  "hash": "a86b5cf299f7f44bd21d763bf1fdb794",
  "result": {
    "markdown": "---\ntitle: \"R resources for Practicum\" \nauthor: \"Sam Weiner\" \ndate: \"1/8/2023\"\ncategories: [resources, rstats]\neditor: visual \ntoc: true \ntoc-depth: 4\nnumber-sections: true\nhighlight-style: pygments\nformat: \n  html: \n    code-link: true\n---\n\n\nHello again! I'm excited to put another post up on Dialectic with Data where I will be brain dumping some of the awesome R resources I have ran across in my journey learning and loving R! This post is was specifically inspired by my practicum team because I have enjoyed sharing these resources with them over time, but many of them have been lost deep in the depths of our slack channel. So, I am going to be cataloging many of the things that I have shared with them, and I will also be adding new resources as I come upon them for them and you!\n\n## General R tips\n\n### R for Data Science\n\nMy biggest and most valuable tip to learning R is read [R for Data Science by Hadley](https://r4ds.had.co.nz/) Wickham and Garret Grolemund.This book has so much knowledge stored inside it. It primarily teaches the [tidyverse](https://www.tidyverse.org/) and its wonderful array of tools for exploratory data analysis and data visualization. It has been a bible for me, and I see myself turning back to it more often than one would think for a self-classified introductory book. The second version is currently under development but available to read [here](https://r4ds.hadley.nz/).\n\n### Rstudio Projects\n\nRstudio projects are a convenient and efficient way of organizing all of the code, data, and visualizations that make up a data science project. The biggest efficiency boost in my experience is that your working directory automatically updates to the folder where your Rstudio project lives, so all of you code and visualizations are automatically saved in the same place. Additionally, any data that you have in, for example, a csv file can be easily uploaded to your environment without having to mess with the working directory. Other important features that are a bit more advanced revolve around Rstudio projects ability to improve reproducibility and use version control.\n\n-   Resources for learning more about Rstudio Projects\n    -   [Intro to Rstudio Projects](https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects)\n    -   [Rstudio and reproducibility](https://rstudio.github.io/renv/articles/renv.html)\n    -   [Rstudio and git](https://happygitwithr.com/index.html)\n\n### Shortcuts, Tips, and Tricks\n\nThe Rstudio IDE is probably the best IDE (integrated development environment) for using R and conducting data science tasks in general. It has stiff competition from the likes of Visual Studio Code, but there are a few tips and tricks that I am going to show you for Rstudio that might make you think twice before switching to a different IDE.\n\n-   `Alt + -`\n    -   Shortcut for inserting the assignment operator `<-`\n-   `Ctrl + Shift + M`\n    -   Shortcut for inserting the pipe magrittr operator `%>%` commonly used in the tidyverse\n    -   Note: As of R 4.1.0, R has a built in pipe operator `|>` which works similarly to the magrittr pipe but has same slight nuances. See [here](https://ivelasq.rbind.io/blog/understanding-the-r-pipe/) and [here](https://stackoverflow.com/questions/67744604/what-does-pipe-greater-than-mean-in-r) for more information on the built in pipe operator.\\\n-   `Ctrl + Shift + R`\n    -   Creates a comment section header in an R source file which is collapsible and also creates an outline to easily navigate your code file.\n-   `Ctrl + Shift + Alt + M`\n    -   Renames a variable in the entire file. Just highlight the variable to be renamed, use the shortcut, and type the new name. Easy as that!\n    -   Using `Ctrl + F` and using replace is another option which is useful when you don't want to rename every instance of a variable in the file.\n-   Code Snippets\n    -   Code Snippets are ways to insert commonly used code structures (or headers) in an R file. Under the *Tools* toolbar, go to *Edit Code Snippets*. There you will see all the code snippets pre-built in Rstudio and and editor for adding new ones. I use this to add a header structure to new scripts where I can fill in information about the script for my teammates to read when they use it.\n-   Multiple Cursors\n    -   This was a tip that I was sorely missing from my time using VS Code until I stumbled upon this simple shortcut in Rstudio! Just by holding down the `Alt` button and clicking or draging with the mouse, you'll have more cursors than you know what to do with. More awesome shortcuts can be found in this [awesome article by Appsilon](https://appsilon.com/rstudio-shortcuts-and-tips/)\n\n### Quarto\n\nQuarto is Rstudio's (now called Posit) new Rmarkdown replacement. The biggest change from Rmarkdown to Quarto is that you can use Quarto in Jupyter Notebooks! Posit (formerly Rstudio) is broadening it's presence in the data science community by engaging with python users in addition to R users. The hope, says Posit, is to create easy tools for cross-language collaboration so that researchers and data scientists who have different languages of choice can work together with ease. From what I have seen, many data scientists who might have preferred R as a language have been pressured to use python because of its large presence in the data science community. This tool is hopefully going to create a pathway for R users to stay R users without complicating team workflows in a python-dominant environment. In addition, Quarto has some awesome features such as helping users easily create documents, presentation, or even blogs (like this one!). So far, it has been very intuitive and the documentation has been very helpful. You can find information at [quarto.org](https://quarto.org).\n\n## Packages\n\nNow lets get to packages! I am going to be listing packages that I think are really helpful (and really cool) for working with data and doing modeling in R.\n\n### Tidyverse\n\n-   [ggplot2](https://ggplot2.tidyverse.org/)\n    -   This package contains the best tools for taking data and making a stellar visualization out of it. It is extremely versatile and at times very complicated. Later on I will be sharing more about ggplot2 and provide some resources for making ggplot2 super easy to understand.\n-   [dplyr](https://dplyr.tidyverse.org/)\n    -   This package has the tools you need to manipulate data. Mutate is useful for feature engineering and summarise is great for calculating summaries of your data set. Using the piping syntax makes this package so powerful. My favorite way of using this package is to dig into the data and pipe what I want into a ggplot!\n-   [readr](https://readr.tidyverse.org/)\n    -   This package makes importing data easy as can be.\n-   [purrr](https://purrr.tidyverse.org/)\n    -   This package contains the tidyverse's toolkit for functional programming. The [iteration chapter](http://r4ds.had.co.nz/iteration.html) in R for Data Science is a great place to start if you are new to functional programming. Another great resources I used is [purrr tutorial](https://jennybc.github.io/purrr-tutorial/index.html) by Jenny Bryan. Functional programming has been very hard for me to learn, but these two resources have really helped me wrap my brain around the concept, and it has already impacted my projects enormously.\n-   [stringr](https://stringr.tidyverse.org/)\n    -   stringr is the handiest way of working with strings in R. It has everything under the sun when it comes to manipulating and cleaning character data.\n-   [forcats](https://forcats.tidyverse.org/)\n    -   Factors are a very common data type in R, but they can be tricky at times. This package has some awesome tools for working with factors that will really come in handy if you use factors a lot.\n\n### Tidymodels\n\nTidymodels is a standalone universe which has everything you need to do machine learning in R. The tidymodels project is headed by the same person that create CARET years back, Max Kuhn, and the goal of tidymodels is the same as CARETS goal: To create a unified interface for machine learning in R. The package makes use of R's rich ecosystem of machine learning model packages but standardizes the interface across all those implementations to make the switch between models seamless. The amount of packages and tools in the tidymodels universe is large and there is too much to say for this blog post. However, I do plan on showing how tidymodels is used in a future blog post, so stay tuned! In the meantime, I am going to point you to some amazing resources if you want to get started now. - [Tidy Modeling with R](https://www.tmwr.org/) is a free book written by Max Kuhn and Julia Silge which is the best resource for getting started with tidymodels and it also has some great information on machine learning in general! - [tidymodels.org](https://www.tidymodels.org/) has some amazing content which will be the preferred resource for people who want a shorter format introduction to tidymodels. - [juilasilge.com](https://juliasilge.com/categories/tidymodels/) is the blog of one of the co-authors of Tidy Modeling with R and she has some great blog posts and youtube videos where she implements the tidymodels packages and gives great explanations of what she is doing.\n\n### Awesome Packages\n\nThe Awesome Packages section is going to be a growing list of packages that I find really cool because they have either changed and/or improved the way I use R as a data science student.\n\n#### broom\n\n[broom](https://broom.tidymodels.org/) is a super cool R package. One of the most annoying aspects of using R for data science is trying to extract the output from the model object. Summaries of the model object are easy enough to get using `summary()`, but to extract the components of model like the coefficients, pvalues, or Rsquared value to use for other operations means digging into the model object and trying and failing to use the right index to find it. broom makes that task no more. By using the `tidy()` function on supported model objects, the output is represented as a data frame which makes extracting what you want so much easier. Rsquared and other metric vaules can be found by using `glance()` and `augment()` can add output like predictions from a new dataset back into the data set from whence it came.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nmpg_lm <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n```\n:::\n\n\n\n::: panel-tabset\n\n## summary()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mpg_lm) # Normal summary output\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ wt + hp + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\nwt          -3.16697    0.74058  -4.276 0.000199 ***\nhp          -0.01804    0.01188  -1.519 0.140015    \ncyl         -0.94162    0.55092  -1.709 0.098480 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,\tAdjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n```\n:::\n:::\n\n\n## tidy()\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(mpg_lm) # tidy() returns coefficients and pvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  38.8       1.79       21.7  4.80e-19\n2 wt           -3.17      0.741      -4.28 1.99e- 4\n3 hp           -0.0180    0.0119     -1.52 1.40e- 1\n4 cyl          -0.942     0.551      -1.71 9.85e- 2\n```\n:::\n:::\n\n\n## glance()\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(mpg_lm) # glance returns model metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.843        0.826  2.51    50.2 2.18e-11     3  -72.7  155.  163.    177.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\n## augment()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# augment with 'newdata = ' returns entire dataset plus fitted and residual values\naugment(mpg_lm, newdata = mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 14\n   .rownames     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with 22 more rows, and 2 more variables: .fitted <dbl>, .resid <dbl>\n```\n:::\n\n```{.r .cell-code}\n# augment without newdata uses training data and returns more metrics\naugment(mpg_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 11\n   .rowna…¹   mpg    wt    hp   cyl .fitted .resid   .hat .sigma .cooksd .std.…²\n   <chr>    <dbl> <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n 1 Mazda R…  21    2.62   110     6    22.8 -1.82  0.0750   2.53 1.15e-2  -0.754\n 2 Mazda R…  21    2.88   110     6    22.0 -1.01  0.0582   2.55 2.67e-3  -0.416\n 3 Datsun …  22.8  2.32    93     4    26.0 -3.16  0.0856   2.48 4.05e-2  -1.32 \n 4 Hornet …  21.4  3.22   110     6    20.9  0.464 0.0533   2.56 5.08e-4   0.190\n 5 Hornet …  18.7  3.44   175     8    17.2  1.53  0.110    2.54 1.29e-2   0.647\n 6 Valiant   18.1  3.46   105     6    20.3 -2.15  0.0699   2.52 1.48e-2  -0.888\n 7 Duster …  14.3  3.57   245     8    15.5 -1.19  0.118    2.55 8.53e-3  -0.506\n 8 Merc 24…  24.4  3.19    62     4    23.8  0.636 0.157    2.55 3.55e-3   0.276\n 9 Merc 230  22.8  3.15    95     4    23.3 -0.496 0.152    2.56 2.06e-3  -0.214\n10 Merc 280  19.2  3.44   123     6    20.0 -0.789 0.0469   2.55 1.27e-3  -0.322\n# … with 22 more rows, and abbreviated variable names ¹​.rownames, ²​.std.resid\n```\n:::\n:::\n\n\n:::\n\n#### janitor\n\n[janitor](https://sfirke.github.io/janitor/index.html) is a key package because it does something that, while we might think is important, is not all that interesting, but is a such a quality of life boost for a data scientist. Cleaning dirty data is numerous ways. Cleaning dirty data is a necessary part of the process to extracting value out a data set, but it is such a time sink. Janitor comes in to provide a myriad of helpful functions to get data people working in the data instead of on the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janitor)\n\n# Data frame with bad column names\ntest <- tribble(\n  ~`Customer ID`, ~RegionCode, ~ElePhanTIntHeRoOm,\n  242, \"A\", 24,\n  3422, \"B\", 353\n)\n\ntest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  `Customer ID` RegionCode ElePhanTIntHeRoOm\n          <dbl> <chr>                  <dbl>\n1           242 A                         24\n2          3422 B                        353\n```\n:::\n\n```{.r .cell-code}\n# clean_names() takes care of most bad column names in most forms\n# Not always going to work but its great for 99% of the data files you'll encounter\njanitor::clean_names(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  customer_id region_code ele_phan_t_int_he_ro_om\n        <dbl> <chr>                         <dbl>\n1         242 A                                24\n2        3422 B                               353\n```\n:::\n:::\n\n\n\n#### skimr\n\n[skimr](https://docs.ropensci.org/skimr/) is your best friend when beginning exploratory data analysis. Using the `skim()` function on a data frame, a really easy to understand output will be generated containing summary statistics for all of your columns. The information will include variable type, percent missing, 0th, 25th, 50th, 75th, 100th percentile value for continuous variables, mean, standard deviation, and more! I think that you will love this tool once you start to use it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(skimr)\n\nskim(mtcars)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |mtcars |\n|Number of rows           |32     |\n|Number of columns        |11     |\n|_______________________  |       |\n|Column type frequency:   |       |\n|numeric                  |11     |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|-----:|------:|------:|------:|------:|:-----|\n|mpg           |         0|             1|  20.09|   6.03| 10.40|  15.43|  19.20|  22.80|  33.90|▃▇▅▁▂ |\n|cyl           |         0|             1|   6.19|   1.79|  4.00|   4.00|   6.00|   8.00|   8.00|▆▁▃▁▇ |\n|disp          |         0|             1| 230.72| 123.94| 71.10| 120.83| 196.30| 326.00| 472.00|▇▃▃▃▂ |\n|hp            |         0|             1| 146.69|  68.56| 52.00|  96.50| 123.00| 180.00| 335.00|▇▇▆▃▁ |\n|drat          |         0|             1|   3.60|   0.53|  2.76|   3.08|   3.70|   3.92|   4.93|▇▃▇▅▁ |\n|wt            |         0|             1|   3.22|   0.98|  1.51|   2.58|   3.33|   3.61|   5.42|▃▃▇▁▂ |\n|qsec          |         0|             1|  17.85|   1.79| 14.50|  16.89|  17.71|  18.90|  22.90|▃▇▇▂▁ |\n|vs            |         0|             1|   0.44|   0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▆ |\n|am            |         0|             1|   0.41|   0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▆ |\n|gear          |         0|             1|   3.69|   0.74|  3.00|   3.00|   4.00|   4.00|   5.00|▇▁▆▁▂ |\n|carb          |         0|             1|   2.81|   1.62|  1.00|   2.00|   2.00|   4.00|   8.00|▇▂▅▁▁ |\n:::\n:::\n\n\n\n#### reprex\n\nWe all know what it's like to feel completely baffled when writing code. At least I can say I have felt completely baffled when writing code and I have felt stuck countless times. And when I am baffled, I go to the internet to find the cure. If I ever have to ask a question on sites like stack overflow or github to ask for help, I always use [reprex](https://reprex.tidyverse.org/) to help me create a minimally reproducible example. A minimally reproducible example is simple piece of code that replicates your problem so that other knowledgeable people can help you diagnose your issue. Asking questions with reprex gives you the best chance to finding an answer to your question because you have helped the internet help you by giving them the exact situation you are facing.\n\nCreate a reproducible example of the error that you are encountering. For instance, lets say you are trying to create three boxplots looking at the distribution of some continuous variable across the values of a categorical variable, but the output doesn't look right. Let's use the mtcars dataset to illustrate this issue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOnce we have recreated our issue in a simple and easy to follow example, we highlight all of the code that was used to create the example, *including the libraries*. Then, we run the `reprex()` function which will nicely format our code with any errors or warnings that we may have gotten and copy that to our clipboard.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reprex)\n\nreprex()\n```\n:::\n\n\nThe output will look like this when we upload our code to stackoverflow or github:\n\n::: {.column-margin}\nIt even includes the plot as image which will show up when we post to stack overflow or github!\n:::\n\n\n``` r\nlibrary(tidyverse)\n\nggplot(mtcars, aes(cyl, mpg)) +\n  geom_boxplot()\n#> Warning: Continuous x aesthetic\n#> ℹ did you forget `aes(group = ...)`?\n```\n\n![](https://i.imgur.com/lrYe9oR.png)<!-- -->\n\n<sup>Created on 2023-01-08 with [reprex v2.0.2](https://reprex.tidyverse.org)</sup>\n\nAlso, by adding `factor()` around `cyl`, we can get the plot we were looking for!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(factor(cyl), mpg)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![Correct way to create boxplot](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n#### here\n\nSharing code is an important part of collaborative data science projects which is common in the field. To be able make sure code is able to be run by others when shared, coders must be cognizant of not hard-coding file paths into their scripts. Other people will 99.99% of the time not have the same file structure as you do, and hard coding a file path will make that script only usable by you until all the file paths are changed by the person you shared it with. We don't want to make sharing code difficult, so I am hoping that you use the [here package](https://here.r-lib.org/) to help you create relative file paths! The here package makes creating relative file paths as simple as can be and using it will make sharing code will be hindrance free.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nhere() starts at C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\n```\n:::\n:::\n\n\nCalling `here()` will output the top of the project directory\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog\"\n```\n:::\n:::\n\n\nBy adding folder names in quatation marks, we can easily dive deeper into our project directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere(\"posts\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts\"\n```\n:::\n\n```{.r .cell-code}\nhere(\"posts\", \"R-resources\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:/Users/sam/Insync/sweiner@ncsu.edu/Google Drive/personal_blog/posts/R-resources\"\n```\n:::\n:::\n\n\n\n#### todor\n\nWorking on big projects means that there are always multiple tasks that are being managed at once. The [todor package](https://github.com/dokato/todor) lets you create searchable comments within your R scripts or even entire projects to help you keep track of your tasks. Never forget again with todor!\n\nUsing todor is as easy writing a comment in R and using a keyword like TODO at the beginning. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(todor)\n\n# TODO Show an example of todor\n```\n:::\n\n\n``` markdown\n<!-- TODO Change this section. -->\n```\n\nWhen we call the `todor()` function, we get output like this. As you might have noticed, todor can detect HTML comments as well! \n\n![todor](todor.png)\n\n\n#### styler\n\nSometimes when we're coding, readability is not on the front of our mind. Use the [styler package](https://styler.r-lib.org/) to help make your scripts more readable so that you know what it's doing the next time you come back to it. All you have to do is run one function, and the package non-invasively stylizes your code to be in a standard format so that your code looks consistent across projects.\n\nLet's say we have code like this. Quite difficult to read through, no?\n\n```r\niris |> \n  filter(Sepal.Length > 5.1) |> select(Petal.Width, Petal.Length) |> group_by(Petal.Length)|> summarise(mean = Petal.Width)\n```\n\nAfter running `styler::style_active_file()`, our code is reformmated and much more readable in my opinion.\n\n```r\niris |>\n  filter(Sepal.Length > 5.1) |>\n  select(Petal.Width, Petal.Length) |>\n  group_by(Petal.Length) |>\n  summarise(mean = Petal.Width)\n```\n\n\n\n## ggplot2\n\n[ggplot2](https://ggplot2.tidyverse.org/) is the primary visualization package in R. It allows for extreme creativity when turning data into visualizations. Checkout the [#tidytuesday](https://twitter.com/search?q=%23tidytuesday&src=hashtag_click) hashtag on twitter to see some of the crazy impressive visualizations made with ggplot2! ggplot2 is not a package that is simple to master, however. For those beginning their journey with ggplot2, I recommend R for Data Science's [chapter on data visualization](https://r4ds.had.co.nz/data-visualisation.html) which gives a great explanation behind the design of ggplot2 and how it is meant to be used. If you need a reference for ggplot2 code, [R Graphics Cookbook](https://r-graphics.org/) is a great resource as are the [R Graph Gallery](https://r-graph-gallery.com/index.html) and [R charts](https://r-charts.com/ggplot2/). I find that sometimes I need to understand how ggplot2 takes data and makes visuals out of it, which is what R for Data Science is great for, and other times I just need a resource to show me all the cool ways I can represent my data, which is what those other three resources are there for.\n\n### Extensions\n\nOne major benefit of ggplot2 being the default visualization package in R is that it has been building blocks of choice for so many R programmers create new visualization capabilities in R and ggplot2. There are hundreds of extension packages for ggplot2 that add onto its functionality in incredible ways. For instance, gganimate creates the ability for you to have animated graphics in R! And the best part is that these extensions are usually very easy to learn because they all use the same ggplot2 mechanics as I was saying earlier. A full list of registered extensions can be found [here](https://exts.ggplot2.tidyverse.org/gallery/). One really exquisite ggplot extension I would like to highlight is [esquisse](https://dreamrs.github.io/esquisse/). This package creates a tableau-like interface inside of Rstudio, so that we can interactively plot our data! It has been a game changer for me when doing EDA.\n\n\n\n![esquisse example](esquisse.png)\n\n### Fonts\n\nOne way that people like to modify their visualizations from base ggplot2 is by modifying text fonts. Packages like [extrafont](https://github.com/wch/extrafont) and [showtext](https://cran.r-project.org/web/packages/showtext/vignettes/introduction.html) make doing this extremely easy. [hrbrthemes](https://cinc.rud.is/web/packages/hrbrthemes/) is an example of pre-built themes that R users have created that take try to enhance ggplot2 output in this way.\n\nHere's what we can do with showtext.\n\nFirst lets look at standard ggplot2 output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nUsing showtext, we can import fonts from google fonts with a simple function and then use some ggplot2 options to include theme in our plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(showtext)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: sysfonts\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: showtextdb\n```\n:::\n\n```{.r .cell-code}\nfont_add_google(\"Dancing Script\", \"dance\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Covered By Your Grace\", \"grace\")\nfont_add_google(\"Rock Salt\", \"rock\")\n\nshowtext_auto()\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  xlab(\"Sepal Length\") +\n  ylab(\"Sepal Width\") +\n  ggtitle(\"Sepal Width by Sepal Length Colored by Species\") +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(family = \"gochi\", size = 20), \n    axis.title.y = element_text(family = \"bell\", size = 20), \n    plot.title = element_text(family = \"grace\", size = 20), \n    legend.text = element_text(family = \"rock\", size = 20), \n    legend.title = element_text(\"dance\", size = 20)\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nWe used four different fonts in one ggplot! I'll admit it's not the nicest looking plot I've ever made, but the point stands that you can customize ggplots to your hearts content using packages like showtext.\n\n### Scales\n\nScaling data in a visualization is a crucial part of effective data communication. If the scale of the data is not clear, then any inferences from that communication will either be unclear or misled. The [scales package](https://scales.r-lib.org/) provides some awesome tooling to make very clear and descriptive plot scales.\n\nHere's a bad example but an example nonetheless. Using two `scales` functions, `label_dollar()` and `label_percent`, we changed the axis labels of the previous graph to be more descriptive. There are many more powerful fuctions in the scales package that I am sure will benefit your data communication tasks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(scales)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'scales'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n## Databases\n\nWorking with databases in R is not terribly complicated. The [DBI](https://dbi.r-dbi.org/) package is very useful when connecting to and querying databases. But there are some really cool tools for working with databases in that I you're going to like.\n\n### dbplyr\n[dbplyr](https://dbplyr.tidyverse.org/) is a tidyverse package that converts dplyr code into SQL queries. Yes, you read that right. I need to note right away, however, that it does not aim to replace SQL in your data science workflow; while it is robust, it is not a complete replacement. With that said, when I have been coding in R for a while and have dplyr code right at the front of mind, it has been great to just whip up some quick code to enable some more EDA with dbplyr. It is a great addition to any R users workflow if they work with databases frequently.\n\n## Causal Inference\n\nCausal inference is a topic that I have been learning about a lot lately. As someone who does not have a formal background in statistics or epidemiology, it has been important to me to find resources that teach causal inference in terms that I can understand. Here is a list of resources and R packages that can help you if you are interested in causal inference.\n\n-   [Causal Inference in R](https://www.r-causal.org/) is a ongoing book project by Malcolm Barrett, Lucy D'Agostino McGowan, and Travis Gerke. It contains some amazing information at the moment and it will only get better as the authors work on it.\n\n-   [Causal Inference for The Brave and True](https://matheusfacure.github.io/python-causality-handbook/landing-page.html) by Matheus Facure Alves is a book that has great explanations of causal inference techniques and concepts and everything coding related is written in python for those interested in a python implementation of causal inference!\n\n-   [Causal Inference: What If?](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2022/11/hernanrobins_WhatIf_13nov22.pdf) by Miguel A. Hernán and James M. Robins is causal inference bible. It is much heavier than the resources above and the focus is purely on the methodology of causal inference. However R code which follows part 2 of the book chapter by chapter can be found [here](https://causalinferencebookr.netlify.app/).\n\n-   [Workshop: Causal Inference in R](https://www.youtube.com/watch?v=n8c-UK19hbA) is a video workshop by Malcolm Barrett and Lucy D'Agostino McGowan. The workshop has been updated since the video but no new video has surfaced that I could find. The new workshop can be found on github [here](https://github.com/malcolmbarrett/causal_inference_r_workshop) with the slides and exercise rmarkdown files. The solutions to the exercises can be found [here](https://github.com/malcolmbarrett/causal_inference_r_workshop_solutions).\n\n-   [MatchIt](https://kosukeimai.github.io/MatchIt/) is a package that creates matches based on observational data. It has awesome vignettes on its website that explains matching for causal inference in detail and has many method options to implement for matching.\n\n-   [ggdag](https://ggdag.malco.io/) is a way to plot directed acyclic dags (DAGs) in R using more a ggplot2-like interface as opposed to [daggity](http://www.dagitty.net/primer/).\n\n-   [propensity](https://github.com/malcolmbarrett/propensity) helps calculate propensity scores and weights for a wide variety of research questions.propensity is under very early development.\n\n-   [tipr](https://lucymcgowan.github.io/tipr/) After fitting your model, you can determine the unmeasured confounder needed to tip your analysis.\n\n-   [halfmoon](https://github.com/malcolmbarrett/halfmoon) The goal of halfmoon is to cultivate balance in propensity score models.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}